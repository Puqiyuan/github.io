#+TITLE: 物理页面分配之快速路径
#+AUTHOR: Cauchy(pqy7172@gmail.com)
#+OPTIONS: ^:nil
#+EMAIL: pqy7172@gmail.com
#+HTML_HEAD: <link rel="stylesheet" href="../../../org-manual.css" type="text/css">
#+OPTIONS: htmlize:nil
本文分析在理想情况下，也就是没有内存短缺时内核通过伙伴系统分配物理页面的流程。物理页面分配的接口包括alloc_pages，这个函数在成功分配时返回的是第一个页面的page数据结构。另外一类接口是__get_free_pages，返回的是内核空间的虚拟地址。本文主要以alloc_pages为入口，分析内核是如何在理想情况下经过快速路径去分配物理页面的，alloc_pages就是一个简单的宏定义，主要调用alloc_pages_noprof。

* alloc_pages_noprof
*原型：*
#+begin_src c
struct page *alloc_pages_noprof(gfp_t gfp, unsigned int order)
#+end_src
*作用：*

分配2^order个连续页面，第一个物理页面自然对齐，所谓自然对齐，举个例子假如order为3，那么就会对齐到2^3*PAGE_SIZE的字节处。当在进程上下文时，会遵循NUMA分配策略。分配失败时返回NULL。

*详细分析：*

参数gfp的类型是gfp_t：
#+begin_src c
typedef unsigned int __bitwise gfp_t;
#+end_src
__bitwise主要是为了类型安全而存在，比如gfp_t类型的量不能和int类型的量进行运算和直接赋值（除非进行了类型强制转换），否则开启了Wall的编译选项时，编译器会报警告，它是编译器支持的一
个attribute。GFP标志主要用来指明内存应该如何分配，比如典型的由GFP推出内存应该在哪个zone中去分配。GFP这个缩写其实是get_free_pages，__开头这样的GFP标志比较底层，一般的用户应该使用GFP_KERNEL这样的由__开头的标志形成的组合。

该函数会根据当前上下文是进程上下文还是中断上下文，分不同的情况传入不同的参数pol（类型为mempolicy）去调用alloc_pages_mpol_noprof函数，alloc_pages_noprof定义如下：
#+begin_src c
struct page *alloc_pages_noprof(gfp_t gfp, unsigned int order)
{
	struct mempolicy *pol = &default_policy;

	/*
	 * No reference counting needed for current->mempolicy
	 * nor system default_policy
	 */
	if (!in_interrupt() && !(gfp & __GFP_THISNODE))
		pol = get_task_policy(current);

	return alloc_pages_mpol_noprof(gfp, order, pol, NO_INTERLEAVE_INDEX,
				       numa_node_id());
}
#+end_src
在不处于中断上下文并且传入的GFP标志位没有__GFP_THISNODE时，内存的NUMA分配策略才会生效。那么如何判定是不是在中断上下文呢，__GFP_THISNODE究竟是什么意思呢，还有其它的NUMA策略标志吗？

判断是否在中断上下文中使用in_interrupt：
#+begin_src c
#define in_interrupt()		(irq_count())
#+end_src
在没有定义CONFIG_PREEMPT_RT的情况下，irq_count被如下定义：
#+begin_src c
# define irq_count()		(preempt_count() & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_MASK))
#+end_src

preempt_count在通用头文件include/asm-generic/preempt.h与架构头文件arch/x86/include/asm/preempt.h中均有定义，但是一般是架构头文件优先使用，这种优先特性体现在Makefile中对头文件使用-I选项包含头文件的先后上，preempt_count在上述两个头文件中均有实现，但是-I选项只要找到第一个有实现的头文件即停止搜索：
#+begin_src makefile
LINUXINCLUDE    := \
		-I$(srctree)/arch/$(SRCARCH)/include \
		-I$(objtree)/arch/$(SRCARCH)/include/generated \
		$(if $(building_out_of_srctree),-I$(srctree)/include) \
		-I$(objtree)/include \
		$(USERINCLUDE)
export KBUILD_CPPFLAGS NOSTDINC_FLAGS LINUXINCLUDE OBJCOPYFLAGS KBUILD_LDFLAGS
#+end_src

X86架构下，preempt_count被如下定义：
#+begin_src c
static __always_inline int preempt_count(void)
{
	return raw_cpu_read_4(pcpu_hot.preempt_count) & ~PREEMPT_NEED_RESCHED;
}
#+end_src
也就是说每个CPU都有一个4字节的int量preempt_count表征现在的抢占计数，这32个bit按如下划分：
#+begin_example
         PREEMPT_MASK:	0x000000ff
         SOFTIRQ_MASK:	0x0000ff00
         HARDIRQ_MASK:	0x000f0000
             NMI_MASK:	0x00f00000
 PREEMPT_NEED_RESCHED:	0x80000000
#+end_example
也就是说最低8个bit（最低1个字节）用来计数抢占，低第二个字节用来表示软中断的计数，依次类推，那么NMI_MASK、HARDIRQ_MASK以及SOFTIRQ_MASK等各种MASK宏用来取出对应字段计数的，就可以按如下代码定出：
#+begin_src c
#define PREEMPT_BITS	8
#define SOFTIRQ_BITS	8
#define HARDIRQ_BITS	4
#define NMI_BITS	4

#define PREEMPT_SHIFT	0
#define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
#define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
#define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)

#define __IRQ_MASK(x)	((1UL << (x))-1)

#define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
#define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
#define NMI_MASK	(__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)
#+end_src

所以回到前面irq_count的定义以及回答如何判定是不是在中断上下文中：只要不可屏蔽中断、硬中断以及软中断三者有其一即可认为当前处于中断上下文中。而一般在进入中断上下文时会对preempt_count相应的字段进行自增：
#+begin_example
__irq_enter->preempt_count_add->__preempt_count_add
#+end_example

__GFP_THISNODE标志主要作用是表明从指定的节点上分配内存，禁止分配回退或使用其它策略，如果请求的节点没有足够的内存资源，那么分配将会失败，这种情况自然不需要考虑NUMA内存分配策略了。除了这个标志还有如下的一些移动和放置策略：

- __GFP_MOVABLE \\
  表示页面是可移动的。这个标志通常用于那些可以在内存整理（compaction）过程中通过页面迁移移动的页面，或是可以被回收的页面。在内存管理中，标记为__GFP_MOVABLE的页面将被放置在特定的pageblocks中，这些pageblocks一般只包含可移动页面，以尽量减少外部碎片的问题。

- __GFP_RECLAIMABLE \\
  主要用于slab分配。指定了SLAB_RECLAIM_ACCOUNT的slab分配使用该标志，这些页面可以通过shrinker机制回收。这使得slab分配的内存可以在系统需要时被回收，以便释放更多的内存资源。

- __GFP_WRITE \\
  表示调用者打算修改页面内容，即页面将被“写脏”（dirty）。内核在分配这些页面时，会尽量将这些页面在本地节点之间进行分散分配，以避免所有脏页集中在同一个内存区域或节点，帮助实现公平的内存分配策略（fair zone allocation policy）。

- __GFP_HARDWALL \\
  强制执行cpuset的内存分配策略。如果系统中存在cpuset配置（用于控制和隔离不同任务的内存使用），这个标志确保页面分配遵循cpuset的内存限制和隔离策略。

- __GFP_ACCOUNT \\
  该标志表示分配的内存将被记账到kmemcg（Kernel Memory Control Group），即为分配的内存计入内核内存控制组。它用于限制和跟踪控制组（cgroup）中分配的内核内存资源。

当既不在中断上下文gfp参数也没有设置__GFP_THISNODE时，就会调用get_task_policy函数：
#+begin_src c
struct mempolicy *get_task_policy(struct task_struct *p)
{
	struct mempolicy *pol = p->mempolicy;
	int node;

	if (pol)
		return pol;

	node = numa_node_id();
	if (node != NUMA_NO_NODE) {
		pol = &preferred_node_policy[node];
		/* preferred_node_policy is not initialised early in boot */
		if (pol->mode)
			return pol;
	}

	return &default_policy;
}
#+end_src
该函数首先获取当前进程的内存分配策略mempolicy，mempolicy可以被关联到一个进程，也可以关联到一个VMA。对于VMA关联的，优先考虑，然后才是进程关联。根据上面get_task_policy函数的定义，内核有一个默认的mempolicy叫default_policy，其定义如下：
#+begin_src c
static struct mempolicy default_policy = {
	.refcnt = ATOMIC_INIT(1), /* never free it */
	.mode = MPOL_LOCAL,
};
#+end_src
MPOL_LOCAL是NUMA内存策略的默认方式，所谓NUMA内存策略可以允许用户指定在特定节点上进行内存分配的优先级和方式，适用于不同的进程或VMA（虚拟内存区域）。这些策略可以用于优化多节点系统上的内存访问效率。具体有以下方式：

+ interleave（交错方式MPOL_INTERLEAVE）\\
  内存分配在指定的一组节点上交错进行，如果分配失败则会采用常规的回退策略。对于VMA分配，这种交错策略基于对象的偏移量（或匿名内存的映射偏移量）；对于进程策略，则基于一个进程计数器进行分配。

+ weighted interleave（加权交错MPOL_WEIGHTED_INTERLEAVE）\\
  类似于interleave，但允许根据每个节点的权重分配内存。例如，nodeset(0,1)与权重(2,1)表示每在节点0上分配两页内存后，再在节点1上分配一页内存。

+ bind（绑定MPOL_BIND）\\
  只在指定的节点集合上分配内存，不采用回退策略。

+ preferred（优先某个节点MPOL_PREFERRED）\\
  首先尝试在指定节点上分配内存，若失败则使用常规回退策略。如果节点设置为NUMA_NO_NODE，则优先在本地CPU上分配内存。通常这类似于默认策略，但在VMA上设置时可以覆盖非默认的进程策略。

+ preferred many（多个节点优先MPOL_PREFERRED_MANY）\\
  与preferred类似，但允许指定多个优先节点，然后再进行回退。

+ default（默认）\\
  优先在本地节点上分配内存，或者在VMA上使用进程策略。这是Linux内核在NUMA系统上一直采用的默认行为。

另外，进程策略适用于该进程上下文中的大多数非中断内存分配，中断则不受策略影响，VMA策略只适用于该VMA中的内存分配。策略应用于系统的高区内存，而不应用于低区和GFP_DMA内存分配。对于共享内存（shmem/tmpfs），策略在所有用户之间共享，即使没有用户映射时也会记住该策略。

中断不会使用当前进程的内存策略，它们总是优先在本地CPU上分配内存。这种设计是为了在中断处理过程中尽可能减少延迟。

对于交错策略来说，在进程上下文中，不需要锁定机制，因为进程只会访问自身的状态，因此没有并发冲突。对于VMA的操作，mmap_lock的读锁（down_read）在一定程度上保护了这些操作，以确保内存映射的一致性。

内存策略mempolicy结构体的释放：内存策略对象通过引用计数来管理生命周期。mpol_put()函数会减少内存策略的引用计数，当引用计数降为零时，该内存策略对象会被释放。这种机制保证了对象只会在不再使用时被释放，避免了内存泄漏。

内存策略mempolicy结构体的复制：mpol_dup()函数用于分配一个新的内存策略，并将指定的内存策略复制到新的内存空间。新创建的内存策略对象的引用计数被初始化为1，表示当前调用者持有该引用。这允许多个内存策略对象彼此独立，同时保证每个对象的生命周期被正确管理。

回到get_task_policy函数，如果进程有内存分配策略mempolicy，则返回这个策略。如果进程没有内存策略，那么就会从系统的全局节点策略数组preferrred_node_policy中去获取内存策略，当然在系统启动早期preferred_node_policy里可能是没有数据的，所以需要判断pol->mode非零，因为preferred_node_policy的定义是static的（被初始化为0）：

#+begin_src c
static struct mempolicy preferred_node_policy[MAX_NUMNODES];
#+end_src

MAX_NUMNODES定义了系统支持的最大NUMA节点数量：
#+begin_src c
#ifdef CONFIG_NODES_SHIFT
#define NODES_SHIFT     CONFIG_NODES_SHIFT
#else
#define NODES_SHIFT     0
#endif
#define MAX_NUMNODES    (1 << NODES_SHIFT)
#+end_src

而NODES_SHIFT的值依据不同的架构有不同的配置，这主要体现在比如arch/x86/Kconfig中有如下代码：
#+begin_src conf
config NODES_SHIFT
	int "Maximum NUMA Nodes (as a power of 2)" if !MAXSMP
	range 1 10
	default "10" if MAXSMP
	default "6" if X86_64
	default "3"
	depends on NUMA
	help
	  Specify the maximum number of NUMA Nodes available on the target
	  system.  Increases memory reserved to accommodate various tables.
#+end_src

这样在编译构建时会自动生成，比如CONFIG_NODES_SHIFT在自动生成的头文件include/generated/autoconf.h中被定义为10，那么MAX_NUMNODES = 1 << 10 = 1024。

get_task_policy中还使用了numa_node_id函数，在定义了CONFIG_USE_PERCPU_NUMA_NODE_ID时该函数定义如下：
#+begin_src c
#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID
DECLARE_PER_CPU(int, numa_node);

#ifndef numa_node_id
/* Returns the number of the current Node. */
static inline int numa_node_id(void)
{
	return raw_cpu_read(numa_node);
}
#endif
#+end_src
该值在cpu启动初始化的流程被初始化：
#+begin_src c
start_secondary->cpu_init->set_numa_node
#ifndef set_numa_node
static inline void set_numa_node(int node)
{
	this_cpu_write(numa_node, node);
}
#endif
#+end_src

* alloc_pages_mpol_noprof
*原型：*
#+begin_src c
struct page *alloc_pages_mpol_noprof(gfp_t gfp, unsigned int order,
		struct mempolicy *pol, pgoff_t ilx, int nid)
#+end_src

*作用：*\\
分配2^order个页面，第一个参数是内存分配标志gfp，最后一个参数nid是前面使用numa_node_id获得的该运行CPU所在的node。针对第四个参数ilx，在交错策略下时，ilx表明是否使用task_struct里的il_prev作为依据来选择内存分配的节点，为NO_INTERLEAVE_INDEX时表明使用task_struct:ilx_prev，而当通过get_vma_policy来获得一个有效的ilx值时就使用这个值来确定如何选择哪个节点来分配内存。第三个参数pol就是前面通过get_task_policy获得的内存策略，当然还有其它的调用路径通过get_vma_policy来获得内存策略，这在本文[[get_vma_policy生成ilx][附节: get_vma_policy生成ilx]]中有介绍。

*详细分析：*\\
alloc_pages_mpol_noprof主要分为三个部分来完成其功能：
+ 第一部分是通过policy_nodemask函数获得在哪个（些）节点上分配内存，由nodemask_t类型的指针nodemask表示，并且返回在哪个目标节点上分配的节点号，由nid表示。

+ 第二部分是针对不同的情况调用不同的分配函数，一种是针对MPOL_PREFERRED_MANY内存策略，调用alloc_pages_preferred_many分配内存，而针对大页内存分配的情况会做一些特殊处理，然后再调用__alloc_pages_node_noprof完成内存分配，最后一种情况就是通过__alloc_pages_noprof完成除前面两种特殊情况的“正常”页面分配。

+ 第三部分是针对交错分配的方式，要更新一些统计信息。以下继续针对这三部分的代码详细分析。

** 第一部分
这部分代码确定分配的节点mask以及目标节点的节点号，代码如下：
#+begin_src c
nodemask_t *nodemask;
nodemask = policy_nodemask(gfp, pol, ilx, &nid);
#+end_src
可以看到主要就是调用了policy_nodemask来确定nodemask以及nid，注意这里的nid是alloc_pages_mpol_noprof的最后一个参数nid的地址，nid的值通过numa_node_id获得代表当前运行CPU所在的节点，这里传入nid的地址，意味着policy_nodemask函数可能会修改nid的值。返回的nodemask表示可以在哪些节点上分配内存，而返回的nid是首选的节点。

policy_nodemask根据前面介绍的NUMA内存策略分几种case来给nodemask和nid赋予不同的值，其原型如下：
#+begin_src c
static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,
				   pgoff_t ilx, int *nid)
#+end_src

*第一种case MPOL_PREFERRED：*
#+begin_src c
	nodemask_t *nodemask = NULL;

	switch (pol->mode) {
	case MPOL_PREFERRED:
		/* Override input node id */
		*nid = first_node(pol->nodes);
		break;
#+end_src
这种情况只设置了优先使用分配内存的节点号，使用first_node去找到pol->nodes里第一个置位的bit的序号，从这里也可以看出pol->nodes这个成员表征了候选的可以用于分配的一些节点，只是说这些节点要按各种不同的策略去选择。

*第二种case MPOL_PREFERRED_MANY：*
#+begin_src c
	case MPOL_PREFERRED_MANY:
		nodemask = &pol->nodes;
		if (pol->home_node != NUMA_NO_NODE)
			*nid = pol->home_node;
		break;
#+end_src
这种情况是有nodemask的，因为perferred many就是在一组优先的节点里分配，并且pol->home_node是有效值时，返回pol里的home_node号作为优先分配的节点。

*第三种case MPOL_BIND：*
#+begin_src c
	case MPOL_BIND:
		/* Restrict to nodemask (but not on lower zones) */
		if (apply_policy_zone(pol, gfp_zone(gfp)) &&
		    cpuset_nodemask_valid_mems_allowed(&pol->nodes))
			nodemask = &pol->nodes;
		if (pol->home_node != NUMA_NO_NODE)
			*nid = pol->home_node;
		/*
		 * __GFP_THISNODE shouldn't even be used with the bind policy
		 * because we might easily break the expectation to stay on the
		 * requested node and not break the policy.
		 */
		WARN_ON_ONCE(gfp & __GFP_THISNODE);
		break;
#+end_src
对于使用MPOL_BIND的内存策略，要限制内存策略不能应用到较低序号的zone，这个判断逻辑由apply_policy_zone函数实现，它的第二个参数，调用gfp_zone由gfp标志里去获得分配内存的目标zone号，也就是获得的目标zone号要大于apply_policy_zone里的一个特别的zone号，这时内存策略才生效。这里主要详细分析下gfp_zone的实现，它决定了内存从哪个zone去分配，较为关键。至于本case的代码较为简单。

在分析gfp_zone函数前，先介绍内核内存管理涉及到的各种类型的zone：
+ ZONE_DMA \\
  区域ZONE_DMA用于适应一些旧式设备，这些设备只能通过DMA（直接内存访问）访问低地址的内存（通常在16MB以下），部分旧设备只能访问某个范围内的物理内存地址。如果要在这些设备上进行DMA操作，内存必须分配在这个特定区域内，比如ISA总线设备或早期的DMA控制器。CONFIG_ZONE_DMA配置选项决定是否启用这个zone。

+ ZONE_DMA32 \\
  区域ZONE_DMA32用于支持能够访问32位地址空间（4GB以下）但不能访问更高地址的设备。一些设备，如32位PCI设备，只能访问4GB以下的内存，因此分配时需要确保内存位于这个范围内。适用于需要低于4G地址范围的64位系统。CONFIG_ZONE_DMA32配置选项决定是否启用该zone。

+ ZONE_NORMAL \\
  区域ZONE_NORMAL包含可被大多数内核和用户进程直接访问的常规物理内存区域。在大多数情况下，DMA操作和内核直接访问都可以在ZONE_NORMAL中完成，这是标准的内存区域。所有常规的内存操作（除非设备或操作对内存有特殊要求）。

+ ZONE_HIGHMEM \\
  ZONE_HIGHMEM用于32位系统中超过直接寻址能力的高地址空间。它通过分页机制（如映射页表条目）来访问高于900MB的内存。32位系统只能直接寻址4GB的地址空间，对于大内存机器而言，这样的空间显得不足。因此，把高地址的物理内存放到ZONE_HIGHMEM中，使内核可以通过间接的方式访问它们。在内存大于4GB的32位系统上，通过页表映射方式访问超出寻址能力的内存区域。启用CONFIG_HIGHMEM后可用。

+ ZONE_MOVABLE \\
  ZONE_MOVABLE主要用于可以迁移的页面，比如用户态内存或页缓存等。在内存卸载和巨页分配时，这个zone有助于提高成功率。通过将可以迁移的内存放在ZONE_MOVABLE，可以确保在需要时有一部分内存可以被迁移或回收，从而进行更灵活的内存管理。例如，大页分配和热插拔内存。用于可热插拔的内存、可移动页面的分配，以及减少不能迁移的分配对系统性能的影响。

  这种类型类似于ZONE_NORMAL，但是不可移动的页面却只能在ZONE_NORMAL。不过有以下一些情况会存在ZONE_MOVABLE有unmovable页面的情况：
  - 长期钉住的页面是不可移动的，因此在ZONE_MOVABLE里不允许长期钉住页面，当页面先钉住然后通过page fault分配物理页面，这种情况没什么问题，页面会来自正确的zone（避开ZONE_MOVABLE），但是当页面被钉住时有可能地址空间里已经有页面位于ZONE_MOVABLE了，这时会将这些页面迁移到不同zone（避开ZONE_MOVABLE）中，迁移要是失败，钉住页面也会失败。

  - memblock对于kernelcore/movablecore区域的设置可能会导致ZONE_MOVABLE里有unmovable的页面。
+ ZONE_DEVICE \\
  ZONE_DEVICE主要用于特殊的内存设备，如NVDIMM（非易失性双列直插内存模块）或其它直接映射到系统内存的设备。随着硬件的演进，现代系统需要支持直接映射到内存的硬件设备，如持久内存。这些设备可以直接与系统内存交互，通常是可持久化的。如使用非易失性存储器（如 NVDIMM）、显存映射等。启用CONFIG_ZONE_DEVICE后可用。

针对笔者目前环境的配置，zone_type的定义如下：
#+begin_src c
  enum zone_type {
          ZONE_DMA,
          ZONE_DMA32,
          ZONE_NORMAL,
          ZONE_MOVABLE,
          ZONE_DEVICE,
          __MAX_NR_ZONES
  };
#+end_src

现在可以介绍gfp_zone函数了，gfp_zone函数的主要作用就是根据传入的gfp标志映射到不同类型的zone，也就是返回zone_type类型的枚举，这些类型前面刚介绍过了。现在看gfp_zone的实现：
#+begin_src c
static inline enum zone_type gfp_zone(gfp_t flags)
{
	enum zone_type z;
	int bit = (__force int) (flags & GFP_ZONEMASK);

	z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) &
					 ((1 << GFP_ZONES_SHIFT) - 1);
	VM_BUG_ON((GFP_ZONE_BAD >> bit) & 1);
	return z;
}
#+end_src
GFP_ZONE_TABLE这个宏类似于一个表（数组），它将不同的GFP标志组合映射到不同的zone_type：
#+begin_src c
#define GFP_ZONE_TABLE ( \
	(ZONE_NORMAL << 0 * GFP_ZONES_SHIFT)				       \
	| (OPT_ZONE_DMA << ___GFP_DMA * GFP_ZONES_SHIFT)		       \
	| (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * GFP_ZONES_SHIFT)	       \
	| (OPT_ZONE_DMA32 << ___GFP_DMA32 * GFP_ZONES_SHIFT)		       \
	| (ZONE_NORMAL << ___GFP_MOVABLE * GFP_ZONES_SHIFT)		       \
	| (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * GFP_ZONES_SHIFT)    \
	| (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * GFP_ZONES_SHIFT)\
	| (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * GFP_ZONES_SHIFT)\
)
#+end_src
这样定义出来这个GFP_ZONE_TABLE其实就是一个数字，只是这个数字按每GFP_ZONES_SHIFT个数目的bit存储一个条目，这个条目通过传入的gfp标志索引，索引到的位置上存放了对应gfp标志优先使用的zone用来分配。

这个宏其实定义了一种zone分配的优先级，举个例子对于条目：
#+begin_src c
ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * GFP_ZONES_SHIFT
#+end_src
就是说当传入的GFP标志既设置了__GFP_MOVABLE又设置了__GFP_HIGHMEM，那么优先在ZONE_MOVABLE中分配，但是这个区域分配失败时，也是可以在__GFP_HIGHMEM中分配的。当然，有一些标志组合是无法满足的，比如同时设置了DMA+HIGHMEM，DMA本来就要求地址在低内存区，当然无法又在高地址区分配。

GFP_ZONES_SHIFT在笔者的环境就是2，也就是说GFP_ZONE_TABLE中每2个比特保存一个条目。通过这种一个数字就实现表格映射的作用，实际也是一种性能优化，通过逻辑运算一个数字的方式是比查表数组访存要高效的。

在没有定义特殊的内存zone（比如DMA，HIGHMEM等）时，都会退化为ZONE_NORMAL：
#+begin_src c
#ifdef CONFIG_HIGHMEM
#define OPT_ZONE_HIGHMEM ZONE_HIGHMEM
#else
#define OPT_ZONE_HIGHMEM ZONE_NORMAL
#endif

#ifdef CONFIG_ZONE_DMA
#define OPT_ZONE_DMA ZONE_DMA
#else
#define OPT_ZONE_DMA ZONE_NORMAL
#endif

#ifdef CONFIG_ZONE_DMA32
#define OPT_ZONE_DMA32 ZONE_DMA32
#else
#define OPT_ZONE_DMA32 ZONE_NORMAL
#endif
#+end_src
再来看gpf_zone函数就简单了首先从flags中拿到低4bit的值：
#+begin_src c
	enum zone_type z;
	int bit = (__force int) (flags & GFP_ZONEMASK);
#+end_src
为什么是低4bit呢，通过GFP_ZONEMASK的定义就知道了：
#+begin_src c
  enum {
          ___GFP_DMA_BIT,
          ___GFP_HIGHMEM_BIT,
          ___GFP_DMA32_BIT,
          ___GFP_MOVABLE_BIT,
          ...
  };
  #define BIT(nr)			(UL(1) << (nr))
  #define ___GFP_DMA		BIT(___GFP_DMA_BIT)
  #define ___GFP_HIGHMEM		BIT(___GFP_HIGHMEM_BIT)
  #define ___GFP_DMA32		BIT(___GFP_DMA32_BIT)
  #define ___GFP_MOVABLE		BIT(___GFP_MOVABLE_BIT)
  #define __GFP_DMA	((__force gfp_t)___GFP_DMA)
  #define __GFP_HIGHMEM	((__force gfp_t)___GFP_HIGHMEM)
  #define __GFP_DMA32	((__force gfp_t)___GFP_DMA32)
  #define __GFP_MOVABLE	((__force gfp_t)___GFP_MOVABLE)  /* ZONE_MOVABLE allowed */
  #define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)
#+end_src
这里可以看到低4个bit其实就是zone修饰符，指明了哪些zone用于分配内存。之前介绍过gfp_t本质是个int，但它具有受限的取值，以限制某些int能进行的运算（范围），但是gfp_t不行。比如上面提到了gfp_t可能的四种值，一个完整的gfp_t值列表如下：
#+begin_src c
enum {
	___GFP_DMA_BIT,
	___GFP_HIGHMEM_BIT,
	___GFP_DMA32_BIT,
	___GFP_MOVABLE_BIT,
	___GFP_RECLAIMABLE_BIT,
	___GFP_HIGH_BIT,
	___GFP_IO_BIT,
	___GFP_FS_BIT,
	___GFP_ZERO_BIT,
	___GFP_UNUSED_BIT,	/* 0x200u unused */
	___GFP_DIRECT_RECLAIM_BIT,
	___GFP_KSWAPD_RECLAIM_BIT,
	___GFP_WRITE_BIT,
	___GFP_NOWARN_BIT,
	___GFP_RETRY_MAYFAIL_BIT,
	___GFP_NOFAIL_BIT,
	___GFP_NORETRY_BIT,
	___GFP_MEMALLOC_BIT,
	___GFP_COMP_BIT,
	___GFP_NOMEMALLOC_BIT,
	___GFP_HARDWALL_BIT,
	___GFP_THISNODE_BIT,
	___GFP_ACCOUNT_BIT,
	___GFP_ZEROTAGS_BIT,
#ifdef CONFIG_KASAN_HW_TAGS
	___GFP_SKIP_ZERO_BIT,
	___GFP_SKIP_KASAN_BIT,
#endif
#ifdef CONFIG_LOCKDEP
	___GFP_NOLOCKDEP_BIT,
#endif
#ifdef CONFIG_SLAB_OBJ_EXT
	___GFP_NO_OBJ_EXT_BIT,
#endif
	___GFP_LAST_BIT
};
#+end_src
所有这些gfp_t类型的值，有些是zone修饰符，有些是内存分配行为，比如___GFP_DIRECT_RECLAIM_BIT允许触发直接内存回收，后面的内存管理分析的系列文章还会遇到这些标志，再在具体的代码上下文分析更为形象。

继续分析gfp_zone函数：
#+begin_src c
	z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) &
					 ((1 << GFP_ZONES_SHIFT) - 1);
#+end_src
将GFP_ZONE_TABLE右移bit * GFP_ZONES_SHIFT，这正是前面定义GFP_ZONE_TABLE时，存放在bit这种标志组合索引处的zone序号，它用来优先为bit这种gfp标志组合分配内存，注意是以2个比特为一个条目，所以最后做与运算，相当于只取最后两位。

回到policy_nodemask函数的第三种MPOL_BIND的情况，apply_policy_zone是说要将内存策略应用在较高点的区域，这个条件是本case下设置nodemask的必要条件，另一个必要条件是内存策略的pol->nodes要和当前进程允许的节点mask有交集（也就是task_struct:mems_allowed）。当然MPOL_BIND下也要将内存策略设置的优先node号带出并返回。

*第四种case MPOL_INTERLEAVE：*\\
根据前面的分析，这种情况就是内存分配在一组节点内交错循环分配：
#+begin_src c
	case MPOL_INTERLEAVE:
		/* Override input node id */
		*nid = (ilx == NO_INTERLEAVE_INDEX) ?
			interleave_nodes(pol) : interleave_nid(pol, ilx);
		break;
#+end_src
可以看到，分两种情况选择使用不同的函数来获得内存分配的优先节点，NO_INTERLEAVE_INDEX主要指示了要不要使用task_struct:il_prev来作为索引选择内存分配的节点，如果使用task_struct:il_prev（也就是ilx参数为-1）则通过interleave_nodes函数来获得内存分配的节点：
#+begin_src c
static unsigned int interleave_nodes(struct mempolicy *policy)
{
	unsigned int nid;
	unsigned int cpuset_mems_cookie;

	/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */
	do {
		cpuset_mems_cookie = read_mems_allowed_begin();
		nid = next_node_in(current->il_prev, policy->nodes);
	} while (read_mems_allowed_retry(cpuset_mems_cookie));

	if (nid < MAX_NUMNODES)
		current->il_prev = nid;
	return nid;
}
#+end_src
do-while循环主要是处理并发情况，此处不讨论了。该函数主要使用next_node_in来获得合适的节点号，也就是在pol->nodes中找到第一个置位的bit的序号，并且是在current->il_prev之后，返回的nid小于MAX_NUMNODES时才会给出nid给调用者。

这里需要简单看下next_node_in的实现，该函数主要是要处理一种临界情况，那就是返回的node id的序号等于MAX_NUMNODES，就又要回到参数srcp的第一个置位的bit的序号：
#+begin_src c
#define next_node_in(n, src) __next_node_in((n), &(src))
static __always_inline unsigned int __next_node_in(int node, const nodemask_t *srcp)
{
	unsigned int ret = __next_node(node, srcp);

	if (ret == MAX_NUMNODES)
		ret = __first_node(srcp);
	return ret;
}
#+end_src
如果不使用task_struct:il_prev，就会调用interleave_nid来获得内存分配的节点号：
#+begin_src c
static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)
{
	nodemask_t nodemask;
	unsigned int target, nnodes;
	int i;
	int nid;

	nnodes = read_once_policy_nodemask(pol, &nodemask);
	if (!nnodes)
		return numa_node_id();
	target = ilx % nnodes;
	nid = first_node(nodemask);
	for (i = 0; i < target; i++)
		nid = next_node(nid, nodemask);
	return nid;
}
#+end_src
该函数以传入的ilx作为索引来选择内存分配的节点，首先ilx需要对pol->nodes里置位的节点数目进行取模得到target，然后通过first_node获得nodemask里第一个置位的节点号nid，最后从nid开始循环target次，找到低target个置位的比特的序号，返回。


*第五种case MPOL_WEIGHTED_INTERLEAVE：* \\
该种情况与case 4类似：
#+begin_src c
	case MPOL_WEIGHTED_INTERLEAVE:
		*nid = (ilx == NO_INTERLEAVE_INDEX) ?
			weighted_interleave_nodes(pol) :
			weighted_interleave_nid(pol, ilx);
		break;
#+end_src
依据ilx参数的不同，而调用不同的函数确定nid并返回。当使用task_struct:il_prev时（也就是ilx无效为-1）调用weighted_interleave_nodes来确定分配内存的节点号，当ilx有效时，就会调用weighted_interleave_nid使用ilx来确定，下面依次分析。

首先是weighted_interleave_nodes函数：
#+begin_src c
static unsigned int weighted_interleave_nodes(struct mempolicy *policy)
{
	unsigned int node;
	unsigned int cpuset_mems_cookie;

retry:
	/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */
	cpuset_mems_cookie = read_mems_allowed_begin();
	node = current->il_prev;
	if (!current->il_weight || !node_isset(node, policy->nodes)) {
		node = next_node_in(node, policy->nodes);
		if (read_mems_allowed_retry(cpuset_mems_cookie))
			goto retry;
		if (node == MAX_NUMNODES)
			return node;
		current->il_prev = node;
		current->il_weight = get_il_weight(node);
	}
	current->il_weight--;
	return node;
}
#+end_src

先抛开if条件里的逻辑不谈，每次进入weighted_interleave_nodes函数，就会返回current这个task_struct结构体里的il_prev，它就是记录了当前用于内存分配的节点号，每分配一次，current->il_weight就会递减，这相当于在il_prev这个内存节点上做了权重，分配il_weight次后就会将这个权重递减为1了。

现在分析if里的逻辑，进入这个if的条件是il_weight递减到0，或者前次使用的内存节点已经不在policy里允许的nodes时。

if里的逻辑主要是更新下一个使用的节点，由next_node_in获得，同时current:il_weight也要更新为当前这个节点的权重，这可以通过get_il_weight去获得：
#+begin_src c
static u8 get_il_weight(int node)
{
	u8 *table;
	u8 weight;

	rcu_read_lock();
	table = rcu_dereference(iw_table);
	/* if no iw_table, use system default */
	weight = table ? table[node] : 1;
	/* if value in iw_table is 0, use system default */
	weight = weight ? weight : 1;
	rcu_read_unlock();
	return weight;
}
#+end_src
从这个函数可以知道weight其实就是来自iw_table里对应节点的权重，它事先通过node_store函数去存放。总结来说，这个函数的功能就是达到前面介绍interleave分配的效果：在一组节点中交错分配内存，并且会按照一定的权重来分配，比如这里体现这点的就是每次进这个函数都会将current->il_weight权重递减，为非零本次分配内存的节点号不会改变，而当这个值递减为0，就会改变分配内存的节点号。

当ilx有效时，是weighted_interleave_nid函数用来确定分配内存的节点：
#+begin_src c
static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)
{
	nodemask_t nodemask;
	unsigned int target, nr_nodes;
	u8 *table;
	unsigned int weight_total = 0;
	u8 weight;
	int nid;

	nr_nodes = read_once_policy_nodemask(pol, &nodemask);
	if (!nr_nodes)
		return numa_node_id();

	rcu_read_lock();
	table = rcu_dereference(iw_table);
	/* calculate the total weight */
	for_each_node_mask(nid, nodemask) {
		/* detect system default usage */
		weight = table ? table[nid] : 1;
		weight = weight ? weight : 1;
		weight_total += weight;
	}

	/* Calculate the node offset based on totals */
	target = ilx % weight_total;
	nid = first_node(nodemask);
	while (target) {
		/* detect system default usage */
		weight = table ? table[nid] : 1;
		weight = weight ? weight : 1;
		if (target < weight)
			break;
		target -= weight;
		nid = next_node_in(nid, nodemask);
	}
	rcu_read_unlock();
	return nid;
}
#+end_src

该函数首先通过一个循环for_each_node_mask将pol->nodes里置上的节点的所有权重求和，然后ilx可以作为一个虚拟地址，其要对weight_total进行取余，这样ilx就会落在权重区间[0, weight_total]。
而后面的while循环实现了将余数target按权重落到相应的区间，其实现方式就是每次只要当前剩余target还不小于当前节点的权重，就会将当前target减去当前节点的权重。举个例子，系统有三个节点，权重依次为3，2，1那么中的权重为6，ilx作为虚拟地址比如可以为0-5，那么这六个地址按照比例一定是有3/6，2/6，1/6的概率分别在第一、二以及三个节点上完成内存分配请求，至于大于5的地址对weight_total取余数后一样符合这个加权概率。

最后这个policy_nodemask函数就是返回前面各个case算出的nodemask，注意nodemask是可以为NULL的，而nid通过最后一个参数带出：
#+begin_src c
	return nodemask;
#+end_src

这样alloc_pages_mpol_noprof函数的第一部分policy_nodemask函数就介绍完了。

# 是对取模操作的一个优化，关于这个优化参见附节[[使用与操作优化取模][附节：使用与操作优化取模操作]]。


** 第二部分
下面介绍alloc_pages_mpol_noprof函数的第二部分，本部分依据不同的情况而调用不同的分配函数。

*情形一MPOL_PREFERRED_MANY*

调用alloc_pages_preferred_many函数：
#+begin_src c
	if (pol->mode == MPOL_PREFERRED_MANY)
		return alloc_pages_preferred_many(gfp, order, nid, nodemask);
#+end_src
#+begin_src c
static struct page *alloc_pages_preferred_many(gfp_t gfp, unsigned int order,
						int nid, nodemask_t *nodemask)
{
	struct page *page;
	gfp_t preferred_gfp;

	/*
	 * This is a two pass approach. The first pass will only try the
	 * preferred nodes but skip the direct reclaim and allow the
	 * allocation to fail, while the second pass will try all the
	 * nodes in system.
	 */
	preferred_gfp = gfp | __GFP_NOWARN;
	preferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);
	page = __alloc_pages_noprof(preferred_gfp, order, nid, nodemask);
	if (!page)
		page = __alloc_pages_noprof(gfp, order, nid, NULL);

	return page;
}
#+end_src
可以看到该种情形通过alloc_pages_preferred_many实现两阶段的页面分配，第一阶段先尝试在nodemask中指定的节点中分配内存，并且不打印分配失败相关的一些信息，也不允许进入直接页面回收，
如果这种分配方式可以成功分配出页面，就返回页面了。否则就会以NULL参数作为nodemask再次调用__alloc_pages_noprof。所以这里可以看到对于指定在某些节点分配内存的方式，先是尽力在这些节点上分配，分配失败时还是会尝试整个系统的节点都可以分配，这是一种回退，尽量保证分配成功有内存可用的方式。

*情形二*

该情形实际是对大页内存分配的一个优化，即保证页面在一个指定的固定节点进行分配：
#+begin_src c
	if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&
	    /* filter "hugepage" allocation, unless from alloc_pages() */
	    order == HPAGE_PMD_ORDER && ilx != NO_INTERLEAVE_INDEX) {
		/*
		 * For hugepage allocation and non-interleave policy which
		 * allows the current node (or other explicitly preferred
		 * node) we only try to allocate from the current/preferred
		 * node and don't fall back to other nodes, as the cost of
		 * remote accesses would likely offset THP benefits.
		 *
		 * If the policy is interleave or does not allow the current
		 * node in its nodemask, we allocate the standard way.
		 */
		if (pol->mode != MPOL_INTERLEAVE &&
		    pol->mode != MPOL_WEIGHTED_INTERLEAVE &&
		    (!nodemask || node_isset(nid, *nodemask))) {
			/*
			 * First, try to allocate THP only on local node, but
			 * don't reclaim unnecessarily, just compact.
			 */
			page = __alloc_pages_node_noprof(nid,
				gfp | __GFP_THISNODE | __GFP_NORETRY, order);
			if (page || !(gfp & __GFP_DIRECT_RECLAIM))
				return page;
			/*
			 * If hugepage allocations are configured to always
			 * synchronous compact or the vma has been madvised
			 * to prefer hugepage backing, retry allowing remote
			 * memory with both reclaim and compact as well.
			 */
		}
	}
#+end_src
首先判断是开启了透明大页并且分配的页面大小为2MB（笔者的环境amd64，HPAGE_PMD_ORDER = 21-12=
9，即分配2^9个页面，2MB大小的空间），而随后的条件ilx != NO_INTERLEAVE_INDEX、pol->mode !=
MPOL_INTERLEAVE以及pol->mode != MPOL_WEIGHTED_INTERLEAVE都是过滤掉交错分配的情况，因为交错
分配的本质要求就是要不固定节点分配，这和这个优化的要求：固定节点分配内存，是相矛盾的，最后
一个条件如果nodemask非空，就要求首选内存节点nid在这个nodemask中。这些条件任一不满足都会走
情形三的标准方式去分配内存。

__GFP_THISNODE指明内存分配应该在nid指明的节点上固定分配，调用者保证未来的内存访问来自nid的cpu，然后内存也是在nid节点分配的，这样本地内存访问的性能优势就得以体现。

*情形三*
改种情形其实没有太多分析的了，就是以前面算好的nodemask和nid，上层函数过来的gfp与order调用__alloc_pages_noprof函数即可：
#+begin_src c
  page = __alloc_pages_noprof(gfp, order, nid, nodemask);
#+end_src


** 第三部分
* 附节
** get_vma_policy生成ilx
# ** 使用与操作优化取模
