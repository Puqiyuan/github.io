<!DOCTYPE html>
<html lang="cn">
<head>
<!-- 2025-05-11 Sun 22:00 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>进程调度</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Cauchy(pqy7172@gmail.com)">
<link rel="stylesheet" href="../../org-manual.css" type="text/css">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">进程调度</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org601847f">1. __schedule</a></li>
<li><a href="#org1d34f97">2. pick_next_task</a></li>
<li><a href="#org6da77ff">3. context_switch</a></li>
</ul>
</div>
</div>
<p>
__schedule是进程调度的核心函数，它先调用pick_next_task函数算出需要运行哪个进程，然后调用context_switch函数完成切换。
</p>

<p>
驱动调度器并进入__schedule这个核心调度函数主要有以下一些方式：
</p>

<ul class="org-ul">
<li>显式阻塞，例如互斥锁（mutex）、信号量（semaphore）、等待队列（waitqueue）等，任务主动睡眠进入等待状态，比如进程调用了mutex_lock、down或wait_event等函数可能会导致自己进入等待状态，需要调度器选择下一个可运行的任务。</li>
<li>定时器的中断处理程序sched_tick可以设置TIF_NEED_RESCHED标志来驱动任务间的抢占调度，该标志会在中断返回和用户态返回路径上被检查。</li>
<li>唤醒任务时并不会直接导致schedule被调用，它只是将任务添加到运行队列（run-queue）中。但如果新加入的任务优先级更高，导致抢占，那么唤醒代码会设置TIF_NEED_RESCHED，然后schedule将在最近的时机被调用，这种时机又分内核是否开启抢占而有所不同：
<ul class="org-ul">
<li>如果内核是可抢占的（CONFIG_PREEMPTION=y）：在系统调用或异常（syscall/exception）上下文中，会在最外层preempt_enable时触发抢占调度，这可能会在wake_up的spin_unlock之后立即发生。在中断（IRQ）上下文中，会在从中断处理程序返回到可抢占上下文时进行调度。</li>
<li>如果内核是不可抢占的（CONFIG_PREEMPTION未启用），那么调度将在以下情况下发生，1显式主动调用cond_resched，2显式主动调用schedule，3从系统调用或异常返回到用户态，4从中断处理程序返回到用户态。</li>
</ul></li>
</ul>

<p>
调用__schedule时必须禁用抢占（preemption disabled），但其父函数可能会处理抢占的问题，比如__schedule__loop就会禁用抢占。
</p>

<p>
__schedule函数的参数sched_mode指明了以何种模式进入的调度器，它的取值可以有：
</p>
<pre class="example" id="orgbc0d515">
/*
 * Constants for the sched_mode argument of __schedule().
 *
 * The mode argument allows RT enabled kernels to differentiate a
 * preemption from blocking on an 'sleeping' spin/rwlock.
 */
#define SM_IDLE			(-1)
#define SM_NONE			0
#define SM_PREEMPT		1
#define SM_RTLOCK_WAIT		2
</pre>
<p>
内核调度器在调度空闲任务时会使用SM_IDLE模式，表示当前没有可运行的普通任务，CPU可能会进入低功耗模式。
</p>

<p>
SM_NONE是普通调度模式，不涉及特殊的抢占或锁等待情况。这是最常见的调度模式，表示当前任务只是正常调度，而不是因为抢占或等待锁。
</p>

<p>
SM_PREEMPT表示任务被抢占（preempted）。当更高优先级的任务就绪时，内核可以抢占当前任务。比如普通任务被抢占式内核（Preemptible Kernel）或实时内核抢占时使用此模式。软实时（SCHED_RR）或硬实时（SCHED_FIFO）任务可能会触发此模式。
</p>

<p>
SM_RTLOCK_WAIT表示任务正在等待RT（实时）锁。实时内核（RT内核）提供了一种机制，使spinlock和rwlock可以在高优先级任务上睡眠（普通内核的自旋锁是不会睡眠的）。当任务因为获取实时锁（RT lock）而阻塞时，会使用这个模式。
</p>

<p>
总的来说这些调度模式主要用于区分不同的调度情况，特别是在抢占式调度和RT内核下，普通任务切换使用SM_NONE，表示任务因为时间片耗尽或显式调度而切换。抢占（Preemption）使用SM_PREEMPT，表示任务因为更高优先级任务到来而被抢占。空闲调度使用SM_IDLE，表示CPU进入空闲模式。实时锁等待使用SM_RTLOCK_WAIT，表示任务在RT互斥锁（如rtmutex）上睡眠等待。
</p>

<p>
比如preempt_schedule_common函数调用_schedule时就指明了参数SM_PREEMPT，表明本次进入调度器是因为抢占。
</p>

<div id="outline-container-org601847f" class="outline-2">
<h2 id="org601847f"><span class="section-number-2">1.</span> __schedule</h2>
<div class="outline-text-2" id="text-1">
<p>
以下是__schedule的实现：
</p>
<pre class="example" id="orge23c531">
static void __sched notrace __schedule(int sched_mode)
{
	struct task_struct *prev, *next;
	/*
	 * On PREEMPT_RT kernel, SM_RTLOCK_WAIT is noted
	 * as a preemption by schedule_debug() and RCU.
	 */
	bool preempt = sched_mode &gt; SM_NONE;
	unsigned long *switch_count;
	unsigned long prev_state;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq-&gt;curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()-&gt;signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up():
	 *
	 * __set_current_state(@state)		signal_wake_up()
	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING)
	 *					  wake_up_state(p, state)
	 *   LOCK rq-&gt;lock			    LOCK p-&gt;pi_state
	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock()
	 *     if (signal_pending_state())	    if (p-&gt;state &amp; @state)
	 *
	 * Also, the membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq-&gt;curr; this
	 * barrier matches a full barrier in the proximity of the membarrier
	 * system call exit.
	 */
	rq_lock(rq, &amp;rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq-&gt;clock_update_flags &lt;&lt;= 1;
	update_rq_clock(rq);
	rq-&gt;clock_update_flags = RQCF_UPDATED;

	switch_count = &amp;prev-&gt;nivcsw;

	/* Task state changes only considers SM_PREEMPT as preemption */
	preempt = sched_mode == SM_PREEMPT;

	/*
	 * We must load prev-&gt;state once (task_struct::state is volatile), such
	 * that we form a control dependency vs deactivate_task() below.
	 */
	prev_state = READ_ONCE(prev-&gt;__state);
	if (sched_mode == SM_IDLE) {
		/* SCX must consult the BPF scheduler to tell if rq is empty */
		if (!rq-&gt;nr_running &amp;&amp; !scx_enabled()) {
			next = prev;
			goto picked;
		}
	} else if (!preempt &amp;&amp; prev_state) {
		try_to_block_task(rq, prev, prev_state);
		switch_count = &amp;prev-&gt;nvcsw;
	}

	next = pick_next_task(rq, prev, &amp;rf);
	rq_set_donor(rq, next);
picked:
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();
#ifdef CONFIG_SCHED_DEBUG
	rq-&gt;last_seen_need_resched_ns = 0;
#endif

	if (likely(prev != next)) {
		rq-&gt;nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq-&gt;curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq-&gt;curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq-&gt;curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC,
		 *   RISC-V.  switch_mm() relies on membarrier_arch_switch_mm()
		 *   on PowerPC and on RISC-V.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 *
		 * The barrier matches a full barrier in the proximity of
		 * the membarrier system call entry.
		 *
		 * On RISC-V, this barrier pairing is also needed for the
		 * SYNC_CORE command when switching between processes, cf.
		 * the inline comments in membarrier_arch_switch_mm().
		 */
		++*switch_count;

		migrate_disable_switch(rq, prev);
		psi_account_irqtime(rq, prev, next);
		psi_sched_switch(prev, next, !task_on_rq_queued(prev) ||
					     prev-&gt;se.sched_delayed);

		trace_sched_switch(preempt, prev, next, prev_state);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &amp;rf);
	} else {
		rq_unpin_lock(rq, &amp;rf);
		__balance_callbacks(rq);
		raw_spin_rq_unlock_irq(rq);
	}
}
</pre>
<p>
该函数本身比较简单，每个cpu都有一关联的runqueues，首先是通过smp_processor_id获取当前运行cpu的编号：
</p>
<pre class="example" id="orgaad58eb">
# define smp_processor_id() __smp_processor_id()
</pre>
<p>
不同架构有不同的__smp_processor_id实现，对于x86架构来说，每个cpu都维护有一个pcpu_hot结构体，里面有cpu_number成员记录了当前运行的cpu号，cpu_number在初始化的时候通过start_kernel-&gt;setup_per_cpu_areas去设置：
</p>
<pre class="example" id="orgfe975fd">
for_each_possible_cpu(cpu) {
        ...
        per_cpu(pcpu_hot.cpu_number, cpu) = cpu;
        ...
}
</pre>
<p>
而对于其它架构比如arm64，则是在当前运行线程中的一个成员进行记录：
</p>
<pre class="example" id="org62661dc">
#define raw_smp_processor_id() (current_thread_info()-&gt;cpu)
</pre>
<p>
有了cpu号，就可以通过cpu_rq获得对应当前运行cpu的rq运行队列了：
</p>
<pre class="example" id="orge115203">
DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
#define cpu_rq(cpu)		(&amp;per_cpu(runqueues, (cpu)))
</pre>
<p>
通过DECLARE就为每个cpu都开辟了rq的空间，这样per_cpu就可以依据cpu号取到对应的rq运行队列。
</p>

<div class="centered-line">sched_debug</div>
<p>
继续分析schedule_debug：
</p>
<pre class="example" id="org191d5e7">
/*
 * Various schedule()-time debugging checks and statistics:
 */
static inline void schedule_debug(struct task_struct *prev, bool preempt)
{
#ifdef CONFIG_SCHED_STACK_END_CHECK
	if (task_stack_end_corrupted(prev))
		panic("corrupted stack end detected inside scheduler\n");

	if (task_scs_end_corrupted(prev))
		panic("corrupted shadow stack detected inside scheduler\n");
#endif

#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
	if (!preempt &amp;&amp; READ_ONCE(prev-&gt;__state) &amp;&amp; prev-&gt;non_block_count) {
		printk(KERN_ERR "BUG: scheduling in a non-blocking section: %s/%d/%i\n",
			prev-&gt;comm, prev-&gt;pid, prev-&gt;non_block_count);
		dump_stack();
		add_taint(TAINT_WARN, LOCKDEP_STILL_OK);
	}
#endif

	if (unlikely(in_atomic_preempt_off())) {
		__schedule_bug(prev);
		preempt_count_set(PREEMPT_DISABLED);
	}
	rcu_sleep_check();
	SCHED_WARN_ON(ct_state() == CT_STATE_USER);

	profile_hit(SCHED_PROFILING, __builtin_return_address(0));

	schedstat_inc(this_rq()-&gt;sched_count);
}
</pre>
<p>
该函数主要是做一些在调度时的debug检查，第一个检查就是看看即将要被切换出去的prev其内核栈顶是否被污染了（笔者环境开了CONFIG_SCHED_STACK_END_CHECK）：
</p>
<pre class="example" id="org69cc7b4">
#ifdef CONFIG_SCHED_STACK_END_CHECK
	if (task_stack_end_corrupted(prev))
		panic("corrupted stack end detected inside scheduler\n");

	if (task_scs_end_corrupted(prev))
		panic("corrupted shadow stack detected inside scheduler\n");
#endif
</pre>
<pre class="example" id="org49ab21c">
#define task_stack_end_corrupted(task) \
		(*(end_of_stack(task)) != STACK_END_MAGIC)
</pre>
<p>
对于开启了CONFIG_THREAD_INFO_IN_TASK配置的end_of_stack实现如下：
</p>
<pre class="example" id="orgddb2168">
static __always_inline unsigned long *end_of_stack(const struct task_struct *task)
{
#ifdef CONFIG_STACK_GROWSUP
	return (unsigned long *)((unsigned long)task-&gt;stack + THREAD_SIZE) - 1;
#else
	return task-&gt;stack;
#endif
}
</pre>
<p>
可以看到这里返回了task的内核栈底，x86架构上内核栈自顶（大地址处）向下（小地址处）生长，而task-&gt;stack通过如下代码分配获取出来就是页面的小地址处，也就是end_of_stack中返回的内核栈结束的地方：
</p>
<pre class="example" id="org99c03be">
static int alloc_thread_stack_node(struct task_struct *tsk, int node)
{
	unsigned long *stack;
	stack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
	stack = kasan_reset_tag(stack);
	tsk-&gt;stack = stack;
	return stack ? 0 : -ENOMEM;
}
</pre>
<p>
alloc_thread_stack_node被dup_task_struct函数在创建进程的时候调用。对于配置了CONFIG_THREAD_INFO_IN_TASK，进程的thread_info结构体就在task_struct里，而不是传统的放到内核栈task-&gt;stack处，这样可以简化内核栈的管理，不用在栈上去处理thread_info的逻辑（比如加偏移取相应的thread_info里的成员）。
</p>

<p>
task_stack_end_corrupted主要就是检查栈底处的值是不是STACK_END_MAGIC，该值同样通过dup_task_struct-&gt;set_task_stack_end_magic去设置：
</p>
<pre class="example" id="orga10b230">
void set_task_stack_end_magic(struct task_struct *tsk)
{
	unsigned long *stackend;

	stackend = end_of_stack(tsk);
	*stackend = STACK_END_MAGIC;	/* for overflow detection */
}
</pre>
<p>
对于顺序下来的over write，若写到了栈底就会在调度出去时被检测发现而panic，调度出是一个恰当的时机，这样避免了下次调度到该进程时遇到一个被破坏了的栈。
</p>

<p>
task_scs_end_corrupted是类似的原理，不再详细介绍。
</p>
<div class="centered-line">sched_debug</div>
<p>
再往下看schedule_debug的实现：
</p>
<pre class="example" id="org5742019">
if (unlikely(in_atomic_preempt_off())) {
	__schedule_bug(prev);
	preempt_count_set(PREEMPT_DISABLED);
}

</pre>
<p>
这个条件大概率是不会满足的，这里主要想分析这个条件是在判断什么：
</p>
<pre class="example" id="org16038ee">
/*
 * Check whether we were atomic before we did preempt_disable():
 * (used by the scheduler)
 */
#define in_atomic_preempt_off() (preempt_count() != PREEMPT_DISABLE_OFFSET)
</pre>
<pre class="example" id="org7e11ef4">
static __always_inline int preempt_count(void)
{
	return raw_cpu_read_4(pcpu_hot.preempt_count) &amp; ~PREEMPT_NEED_RESCHED;
}
</pre>
<pre class="example" id="orgca265db">
/*
 * The preempt_count offset after preempt_disable();
 */
#if defined(CONFIG_PREEMPT_COUNT)
# define PREEMPT_DISABLE_OFFSET	PREEMPT_OFFSET
#else
# define PREEMPT_DISABLE_OFFSET	0
#endif
</pre>
<pre class="example" id="orgd6d6ae2">
#define PREEMPT_OFFSET	(1UL &lt;&lt; PREEMPT_SHIFT)
</pre>
<pre class="example" id="org3fd584e">
#define PREEMPT_SHIFT	0
</pre>
<p>
对于开启了CONFIG_PREEMPT_COUNT来说，在x86上就是检查preempt_count必须为1，这样条件就是满足，不会进入__schedule_bug，这其实是说运行到此处调度器的代码时，父函数已经调用过一次preempt_disable，而在调用它之前，preempt_count必须为0，也就是不处于原子上下文或已经调用过了preempt_disable，换言之，原子上下文（比如软硬中断中）不允许调度发生。
</p>

<p>
继续往下看一个warn判断：
</p>
<pre class="example" id="org9e2a1b5">
SCHED_WARN_ON(ct_state() == CT_STATE_USER);
</pre>
<p>
ct_state函数在启用CONFIG_CONTEXT_TRACKING_USER配置时会返回有意义的值，它主要用来追踪cpu的当前上下文，可能的状态有：
</p>
<pre class="example" id="orga848afc">
enum ctx_state {
	CT_STATE_DISABLED	= -1,	/* returned by ct_state() if unknown */
	CT_STATE_KERNEL		= 0,
	CT_STATE_IDLE		= 1,
	CT_STATE_USER		= 2,
	CT_STATE_GUEST		= 3,
	CT_STATE_MAX		= 4,
};
</pre>
<p>
这些状态主动在相应上下文切换的代码流程里被切换，RCU可以利用这些状态确认当前CPU是否处于用户态，以便判断是否可以将该CPU视为“非活跃”，从而进行RCU回收，另外在某些架构（如NO_HZ_FULL模式）中，内核会在用户态禁用定时器中断，以减少上下文切换的开销。但是，这样会导致cputime统计变得不准确，因为CPU进入用户态后不会有定时器中断来更新时间统计。CONTEXT_TRACKING_USER通过显式追踪进入/退出用户态的时间，使CPU时间统计能够在NO_HZ_FULL模式下仍然保持准确。
</p>

<p>
至于本warn判断本身，调度器（schedule）运行时一定是在内核态，如果ct_state() == CT_STATE_USER，说明context tracking机制出了问题，所以这里加了SCHED_WARN_ON作为一个调试检查。
</p>

<p>
继续往下看是增加一个profile计数：
</p>
<pre class="example" id="org6d17a47">
profile_hit(SCHED_PROFILING, __builtin_return_address(0));
</pre>
<pre class="example" id="orgd9ce570">
/*
 * Single profiler hit:
 */
static inline void profile_hit(int type, void *ip)
{
	/*
	 * Speedup for the common (no profiling enabled) case:
	 */
	if (unlikely(prof_on == type))
		profile_hits(type, ip, 1);
}
</pre>
<p>
可以进行profiling的包括三个模块：
</p>
<pre class="example" id="orge2533be">
#define CPU_PROFILING	1
#define SCHED_PROFILING	2
#define KVM_PROFILING	4
</pre>
<p>
prof_on在启动时依据profile=的参数设置调用profile_setup来填为上面三个类型中的一个。__builtin_return_address是编译器内置的函数，可以返回当前函数的返回地址。
profile_hits实现如下：
</p>
<pre class="example" id="orgd5791f4">
static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
{
	unsigned long pc;
	pc = ((unsigned long)__pc - (unsigned long)_stext) &gt;&gt; prof_shift;
	if (pc &lt; prof_len)
		atomic_add(nr_hits, &amp;prof_buffer[pc]);
}
</pre>
<p>
prof_buffer是在初始化函数profile_init里开的空间，可以看到do_profile_hits就是记录了内核text段某个pc执行的次数，是一种性能统计的功能，它实现在CONFIG_PROFILING配置下，并可以通过/proc访问文件的方式。
</p>

<p>
最后是schedstat_inc自增rq的sched_count，当然这需要开启CONFIG_SCHEDSTATS配置，这样/proc/schedstat就可以反应一些调度统计信息了：
</p>
<pre class="example" id="org49061f4">
schedstat_inc(this_rq()-&gt;sched_count);
</pre>
<pre class="example" id="org93785b3">
#define   schedstat_inc(var)		do { if (schedstat_enabled()) { var++; } } while (0)
</pre>
<p>
注意这里是宏展开，不是函数调用，所以最后对sched_count的自增一定会生效，展开就是文本替换，在预编译阶段完成：
</p>
<pre class="example" id="org45fe98f">
#define   schedstat_inc(var)		do { if (schedstat_enabled()) { this_rq()-&gt;sched_count++; } } while (0)
</pre>
<div class="centered-line">sched_debug</div>

<p>
回到__schedule，继续往下看：
</p>
<pre class="example" id="org1cb2fe4">
if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
    hrtick_clear(rq);
</pre>

<p>
HRTICK是高精度调度定时器功能，默认情况下这个功能是关闭的，但是sched_feat宏本身是有定义的，尤其是笔者的环境开了CONFIG_SCHED_DEBUG以及CONFIG_JUMP_LABEL两个配置，sched_feat使用静态分支判断的方法：
</p>
<pre class="example" id="org181c88a">
#define sched_feat(x) (static_branch_##x(&amp;sched_feat_keys[__SCHED_FEAT_##x]))
</pre>
<p>
通过sched_feat这里主要是想介绍上面提到的两个配置的功能，至于HRTICK这个调度器feature本身，默认是不开启的：
</p>
<pre class="example" id="orgd520721">
SCHED_FEAT(HRTICK, false)
</pre>

<p>
CONFIG_SCHED_DEBUG开启了调度器的调试信息，这样就会在/sys/kernel/debug/sched/下展示很多关于调度器的信息，开启这个代价很小，所以笔者环境默认开启了，比如/sys/kernel/debug/sched/features可以查看当前scheduler开启哪些feature：
</p>
<pre class="example" id="org9f2f91a">
PLACE_LAG PLACE_DEADLINE_INITIAL PLACE_REL_DEADLINE RUN_TO_PARITY PREEMPT_SHORT NO_NEXT_BUDDY PICK_BUDDY CACHE_HOT_BUDDY DELAY_DEQUEUE DELAY_ZERO WAKEUP_PREEMPTION NO_HRTICK NO_HRTICK_DL NO_DOUBLE_TICK NONTASK_CAPACITY TTWU_QUEUE SIS_UTIL NO_WARN_DOUBLE_CLOCK RT_PUSH_IPI NO_RT_RUNTIME_SHARE NO_LB_MIN ATTACH_AGE_LOAD WA_IDLE WA_WEIGHT WA_BIAS UTIL_EST NO_LATENCY_WARN
</pre>
<p>
前面有NO的就是没有开启的，比如NO_HRTICK，当然在运行时可以动态的修改这个文件以开启某个feature。另外要介绍的一个配置就是CONFIG_JUMP_LABEL，它优化了almost-always-true和almost-always-false这样的分支预测，传统上，这样的分支预测还是有cmp指令进行比较并决定是否跳转的，这样在硬件上就有分支预测，但是一旦分支预测错误面临的性能损失比较大，jump label优化的做法是一开始都将这样的分支预测编译为nop指令，然后在运行时，通过static_key_enable/disable这样的接口一路向下到text_poke去动态的调整nop为jmp到对应函数（或调整为nop，表示条件不满足）。
</p>

<p>
至于上面的条件自然是不满足就不深入分析了。
</p>

<p>
接着后面是关闭本地cpu的中断响应，local_irq_disable在x86架构上使用的是cli指令，它只会禁止对可屏蔽的外部中断的响应，而异常和NMI（不可屏蔽中断）还是会响应的。
</p>

<p>
然后通过rq_lock获取操作runqueues的自旋锁，也就是操作rq需要有锁保护，防止并发操作带来的数据一致性问题。
</p>

<p>
继续看对smp_mb__after_spinlock的调用，它是针对arm64这样的弱一致内存模型的同步操作，因为在调用__schedule前通常通过__set_current_state设置了进程状态比如为TASK_INTERRUPTIBLE，而另外的独立路径可能设置进程有信号需要处理，比如通过set_tsk_thread_flag(p, TIF_SIGPENDING)，在之前一样的调用__schedule的路径下，通常也会调用signal_pending_state去检查进程是否有需要待处理的pending信号：
</p>
<pre class="example" id="org50120de">
static inline int signal_pending_state(unsigned int state, struct task_struct *p)
{
	if (!(state &amp; (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
		return 0;
	if (!signal_pending(p))
		return 0;

	return (state &amp; TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
}
</pre>
<pre class="example" id="org2351dd9">
static inline int task_sigpending(struct task_struct *p)
{
	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
}

static inline int signal_pending(struct task_struct *p)
{
	/*
	 * TIF_NOTIFY_SIGNAL isn't really a signal, but it requires the same
	 * behavior in terms of ensuring that we break out of wait loops
	 * so that notify signal callbacks can be processed.
	 */
	if (unlikely(test_tsk_thread_flag(p, TIF_NOTIFY_SIGNAL)))
		return 1;
	return task_sigpending(p);
}
</pre>
<p>
可以看到在能够检查另外独立路径设置的TIF_SIGPENDING前（通过signal_pending-&gt;task_sigpending），signal_pending_state需要先检查进程状态是否为TASK_INTERRUPTIBLE，如果进程不处于可中断的状态，根本不会进一步检查是否有pending的信号，这样独立路径设置的需要处理的TIF_SIGPENDING标志，就因为TASK_INTERRUPTIBLE没有及时写入而错过了处理。
</p>

<p>
先检查进程为TASK_INTERRUPTIBLE是因为只有可中断状态，才有意义可以接受待处理的信号，而TASK_UNINTERRUPTIBLE是不响应信号的。
</p>

<p>
独立路径写入TIF_SIGPENDING只是可能引起这个问题表现的条件/场景，不是根本原因，根本原因在于TASK_INTERRUPTIBLE的写入与TIF_SIGPENDING读取之间的并发问题，导致signal_pending_state可能在错误的时间点返回错误的结果。
</p>

<div class="centered-line">update_rq_clock</div>

<p>
调用update_rq_clock前对clock_update_flags左移一位，clock_update_flags有三种取值可能：
</p>
<pre class="example" id="org92e3453">
/*
 * rq::clock_update_flags bits
 *
 * %RQCF_REQ_SKIP - will request skipping of clock update on the next
 *  call to __schedule(). This is an optimisation to avoid
 *  neighbouring rq clock updates.
 *
 * %RQCF_ACT_SKIP - is set from inside of __schedule() when skipping is
 *  in effect and calls to update_rq_clock() are being ignored.
 *
 * %RQCF_UPDATED - is a debug flag that indicates whether a call has been
 *  made to update_rq_clock() since the last time rq::lock was pinned.
 *
 * If inside of __schedule(), clock_update_flags will have been
 * shifted left (a left shift is a cheap operation for the fast path
 * to promote %RQCF_REQ_SKIP to %RQCF_ACT_SKIP), so you must use,
 *
 *	if (rq-clock_update_flags &gt;= RQCF_UPDATED)
 *
 * to check if %RQCF_UPDATED is set. It'll never be shifted more than
 * one position though, because the next rq_unpin_lock() will shift it
 * back.
 */
#define RQCF_REQ_SKIP		0x01
#define RQCF_ACT_SKIP		0x02
#define RQCF_UPDATED		0x04
</pre>
<p>
其中取值为RQCF_REQ_SKIP时，再左移一位就是RQCF_ACT_SKIP，这个值在update_rq_clock里会引起它的直接返回，也就是说内核其它地方可以通过将clock_update_flags设置为RQCF_REQ_SKIP以指示__schedule函数里调用update_rq_clock实际不进行调度时钟值的更新，这样可以省去一部分开销，当然在设置为RQCF_REQ_SKIP的路径上就必然调用过uodate_rq_clock对时钟进行了更新。
</p>

<p>
接下来分析下update_rq_clock函数：
</p>
<pre class="example" id="org8624dfe">
void update_rq_clock(struct rq *rq)
{
	s64 delta;
	u64 clock;

	lockdep_assert_rq_held(rq);

	if (rq-&gt;clock_update_flags &amp; RQCF_ACT_SKIP)
		return;

#ifdef CONFIG_SCHED_DEBUG
	if (sched_feat(WARN_DOUBLE_CLOCK))
		SCHED_WARN_ON(rq-&gt;clock_update_flags &amp; RQCF_UPDATED);
	rq-&gt;clock_update_flags |= RQCF_UPDATED;
#endif
	clock = sched_clock_cpu(cpu_of(rq));
	scx_rq_clock_update(rq, clock);

	delta = clock - rq-&gt;clock;
	if (delta &lt; 0)
		return;
	rq-&gt;clock += delta;

	update_rq_clock_task(rq, delta);
}
</pre>
<p>
这里可以看到如果clock_update_flags为RQCF_ACT_SKIP，那么就直接返回了，而不更新rq里的对应时钟，然后本次更新了时钟，就将clock_update_flags设置为RQCF_UPDATED。
</p>

<p>
update_rq_clock函数通过sched_clock_cpu获得当前的时间戳（在x86架构上一般就是rdtsc指令），和上次记录的clock相减后得到本次增加的delta时间，关于sched_clock_cpu的实现涉及内核clock模块，参见笔者其它文章介绍。
</p>

<p>
在这里简单介绍下cpu_of的实现：
</p>
<pre class="example" id="orge32648e">
static inline int cpu_of(struct rq *rq)
{
#ifdef CONFIG_SMP
	return rq-&gt;cpu;
#else
	return 0;
#endif
}
</pre>
<p>
调度初始化函数sched_init里完成了rq-&gt;cpu的设置：
</p>
<pre class="example" id="org2661255">
for_each_possible_cpu(i) {
	rq-&gt;cpu = i;
}
</pre>
<p>
随后的scx_rq_clock_update是在配置了CONFIG_SCHED_CLASS_EXT时有有效定义，该配置主要是一个基于BPF（Berkeley Packet Filter）的可扩展调度框架，旨在让开发者能够快速编写、部署和实验新的调度策略，而不需要修改Linux内核的核心代码。可以像开发普通BPF程序一样调整调度逻辑，无需重新编译内核，应用程序可以定义专属的CPU调度策略，提高性能，做到无中断地切换调度策略，而不需要重启系统或重编译内核。具体的实现细节可以参考笔者其它文章。
</p>

<p>
rq-&gt;clock是真实的物理时间增量，所以直接往上增加delta，但update_rq_clock_task里更新的rq里的成员需要对delta进行调整：
</p>

<pre class="example" id="orgc51a354">
static void update_rq_clock_task(struct rq *rq, s64 delta)
{
/*
 * In theory, the compile should just see 0 here, and optimize out the call
 * to sched_rt_avg_update. But I don't trust it...
 */
	s64 __maybe_unused steal = 0, irq_delta = 0;

#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	if (irqtime_enabled()) {
		irq_delta = irq_time_read(cpu_of(rq)) - rq-&gt;prev_irq_time;

		/*
		 * Since irq_time is only updated on {soft,}irq_exit, we might run into
		 * this case when a previous update_rq_clock() happened inside a
		 * {soft,}IRQ region.
		 *
		 * When this happens, we stop -&gt;clock_task and only update the
		 * prev_irq_time stamp to account for the part that fit, so that a next
		 * update will consume the rest. This ensures -&gt;clock_task is
		 * monotonic.
		 *
		 * It does however cause some slight miss-attribution of {soft,}IRQ
		 * time, a more accurate solution would be to update the irq_time using
		 * the current rq-&gt;clock timestamp, except that would require using
		 * atomic ops.
		 */
		if (irq_delta &gt; delta)
			irq_delta = delta;

		rq-&gt;prev_irq_time += irq_delta;
		delta -= irq_delta;
		delayacct_irq(rq-&gt;curr, irq_delta);
	}
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	if (static_key_false((&amp;paravirt_steal_rq_enabled))) {
		u64 prev_steal;

		steal = prev_steal = paravirt_steal_clock(cpu_of(rq));
		steal -= rq-&gt;prev_steal_time_rq;

		if (unlikely(steal &gt; delta))
			steal = delta;

		rq-&gt;prev_steal_time_rq = prev_steal;
		delta -= steal;
	}
#endif

	rq-&gt;clock_task += delta;

#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
	if ((irq_delta + steal) &amp;&amp; sched_feat(NONTASK_CAPACITY))
		update_irq_load_avg(rq, irq_delta + steal);
#endif
	update_rq_clock_pelt(rq, delta);
}
</pre>
<p>
CONFIG_IRQ_TIME_ACCOUNTING配置控制的代码主要是要从任务的运行时间里扣除其用于中断的时间，delayacct_irq的实现还可以将这个花费在中断上的时间（也就是由irq引起的delay）给统计到task_struct::delays::irq_delay，当然前提是开启了CONFIG_TASK_DELAY_ACCT，当然开启这个配置能统计到的delay不只是irq，还有比如blkio，swap等。
</p>

<p>
一个问题是如果这个中断就是任务引起的，是不是也应当属于任务自己的时间。
</p>

<p>
CONFIG_PARAVIRT_TIME_ACCOUNTING主要针对虚拟化环境下，如果一个VCPU被Hypervisor抢占，那么需要从运行的delta时间里扣除这部分，因为实际上这部分时间任务并没有运行，使得进程的CPU使用率更加准确。
</p>

<p>
CONFIG_HAVE_SCHED_AVG_IRQ在笔者环境一般没有配置。
</p>

<p>
最后update_rq_clock_task-&gt;update_rq_clock_pelt里会按cpu的算力以及频率对delta物理时间进行缩放，也就是经过相同的delta时间，算力强频率高的cpu实际负载更大：
</p>
<pre class="example" id="org8df64ad">
/*
 * The clock_pelt scales the time to reflect the effective amount of
 * computation done during the running delta time but then sync back to
 * clock_task when rq is idle.
 *
 *
 * absolute time   | 1| 2| 3| 4| 5| 6| 7| 8| 9|10|11|12|13|14|15|16
 * @ max capacity  ------******---------------******---------------
 * @ half capacity ------************---------************---------
 * clock pelt      | 1| 2|    3|    4| 7| 8| 9|   10|   11|14|15|16
 *
 */
static inline void update_rq_clock_pelt(struct rq *rq, s64 delta)
{
	if (unlikely(is_idle_task(rq-&gt;curr))) {
		_update_idle_rq_clock_pelt(rq);
		return;
	}

	/*
	 * When a rq runs at a lower compute capacity, it will need
	 * more time to do the same amount of work than at max
	 * capacity. In order to be invariant, we scale the delta to
	 * reflect how much work has been really done.
	 * Running longer results in stealing idle time that will
	 * disturb the load signal compared to max capacity. This
	 * stolen idle time will be automatically reflected when the
	 * rq will be idle and the clock will be synced with
	 * rq_clock_task.
	 */

	/*
	 * Scale the elapsed time to reflect the real amount of
	 * computation
	 */
	delta = cap_scale(delta, arch_scale_cpu_capacity(cpu_of(rq)));
	delta = cap_scale(delta, arch_scale_freq_capacity(cpu_of(rq)));

	rq-&gt;clock_pelt += delta;
}
</pre>
<pre class="example" id="org2120735">
#define cap_scale(v, s)		((v)*(s) &gt;&gt; SCHED_CAPACITY_SHIFT)
</pre>
<p>
update_rq_clock_pelt按频率以及算力缩放的值放在rq::clock_pelt量里。
</p>
<div class="centered-line">update_rq_clock</div>
<p>
arch_scale_cpu_capacity用于获取cpu的计算能力：
</p>
<pre class="example" id="org98da1d4">
unsigned long arch_scale_cpu_capacity(int cpu)
{
	if (static_branch_unlikely(&amp;arch_hybrid_cap_scale_key))
		return READ_ONCE(per_cpu_ptr(arch_cpu_scale, cpu)-&gt;capacity);

	return SCHED_CAPACITY_SCALE;
}
</pre>
<p>
hybrid一般就是针对arm架构开启了big.LITTLE，而针对Intel x86就是P-core/E-core，针对笔者的配置，就是返回默认的SCHED_CAPACITY_SCALE，也就是没有区分不同cpu的算力：
</p>
<pre class="example" id="org60f6b1f">
# define SCHED_FIXEDPOINT_SHIFT		10
# define SCHED_CAPACITY_SHIFT		SCHED_FIXEDPOINT_SHIFT
# define SCHED_CAPACITY_SCALE		(1L &lt;&lt; SCHED_CAPACITY_SHIFT)
</pre>
<p>
这里可以看到，固定CPU算力就是1024，第一次通过cap_scale对算力进行scale，不过cpu_scale又右移动了，等于delta过的多少时间就是多少负载，因为算力一样。
</p>

<p>
第二个arch_scale_freq_capacity是根据频率来缩放delta时间，这个宏对于x86架构来说就是读取percpu变量arch_freq_scale，其设置会经由scale_freq_tick函数调用this_cpu_write，本质上是MSR_IA32_APERF和MSR_IA32_MPERF这两个msr寄存器分别各自做出差值delta，将delta差值做比值得商，写入arch_scale_freq_capacity是为当前频率需要缩放的比例，MSR_IA32_APERF（Actual PerformanceFrequency）是实际实际工作的时钟周期数，而MSR_IA32_MPERF（Maximum Performance Frequency）是理论最大工作周期数。比值商大于1代表当前处于turbo模式，睿频运行，小于1代表没有满频率运行。
</p>

<p>
注意scale_freq_tick里通过check_shl_overflow先将acnt（MSR_IA32_APERF）乘以2**20进行放大，然后通过check_mul_overflow将mcnt（MSR_IA32_MPERF）乘以2**10也进行放大（假如没有开启hybrid异构算力架构），最终其实是把delta(acnt)和delta(mcnt)的比值本来是个小数，乘以了1024，这样就是1024是比值1，小于1024的代表小数，大于1024的话代表比值大于1，cpu频率处于turbo状态。这是内核优化小数运行的常见手段，将小数乘以一个1024这样整数，避免内核做浮点运算，当然最小精度只有1/1024，可以调整为2048等等大的数值，可以将更小的小数表示到整数范围，也可以做截断处理，当发现小数小于1/1024，直接赋值为0。
</p>

<p>
最后的rq::clock_pelt是两者做了scale的和。
</p>

<p>
回到__schedule继续分析，先取出了nivcsw，这个计数表示非自愿上下文切换（non-voluntary
context switch），“非自愿”的意思是任务被抢占或被强制切换了。参数sched_mode表示了调度模式，最后preempt保存了是否是抢占模式调度，SM_PREEMPT代表抢占模式。prev-&gt;__state是任务的状态，比如TASK_RUNNING、TASK_INTERRUPTIBLE等。
</p>

<p>
如果是因为空闲调度模式进入调度器，那么表示这时可能没有其它任务要执行了，并且rq运行队列里也没有可运行的任务了，并且没有启用BPF调度器时，那么实际没有切换任务，因为next = prev，然后就直接goto picked了，不用经过pick_next_task去挑选下一个需要运行的任务了，而如果不是抢占切换，这代表当前任务主动让出cpu想要block阻塞自己，那么调用try_to_block_task将当前任务标记为阻塞状态，并从运行队列里移除，这种情况下要切换计数的指针为nvcsw，表示自愿上下文切换，因为后面要自增这个计数，到底自增哪个计数，需要根据不同情况选择。
</p>
</div>
</div>

<div id="outline-container-org1d34f97" class="outline-2">
<h2 id="org1d34f97"><span class="section-number-2">2.</span> pick_next_task</h2>
<div class="outline-text-2" id="text-2">
<p>
pick_next_task的实现依据是否开启CONFIG_SCHED_CORE配置有不同的实现，这个配置是支持SMT的，比如Intel的Hyper-Threading，一般没有开启这个配置，这样它实现如下：
</p>
<pre class="example" id="org54111ef">
static struct task_struct *
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	return __pick_next_task(rq, prev, rf);
}

/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	rq-&gt;dl_server = NULL;

	if (scx_enabled())
		goto restart;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those lose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely(!sched_class_above(prev-&gt;sched_class, &amp;fair_sched_class) &amp;&amp;
		   rq-&gt;nr_running == rq-&gt;cfs.h_nr_queued)) {

		p = pick_next_task_fair(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto restart;

		/* Assume the next prioritized class is idle_sched_class */
		if (!p) {
			p = pick_task_idle(rq);
			put_prev_set_next_task(rq, prev, p);
		}

		return p;
	}

restart:
	prev_balance(rq, prev, rf);

	for_each_active_class(class) {
		if (class-&gt;pick_next_task) {
			p = class-&gt;pick_next_task(rq, prev);
			if (p)
				return p;
		} else {
			p = class-&gt;pick_task(rq);
			if (p) {
				put_prev_set_next_task(rq, prev, p);
				return p;
			}
		}
	}

	BUG(); /* The idle class should always have a runnable task. */
}
</pre>
<p>
该函数主要分两部分，restart前的部分是针对CFS调度的优化，就是通用rq里正在运行的所有进程数量等于CFS队列里排队的进程数量并且即将被换出去的进程prev所属的调度类优先级不高于fair_sched_class类时，这样说明当前cpu的rq里只有CFS的进程，那么直接调用pick_next_task_fair选合适的进程来运行即可，这算是一种优化，不用每次都遍历所有调度类。
</p>

<p>
如果不能满足上述条件，那么就会走restart标签的代码去遍历所有调度类找到一个合适的进程去运行，下面针对这两部分分别详细分析。
</p>

<p>
先看下sche_class_above的实现：
</p>
<pre class="example" id="orgc27093c">
#define sched_class_above(_a, _b)	((_a) &lt; (_b))
</pre>
<p>
sched_class_above就是判断即将要调度出去的prev所属的调度类prev-&gt;sched_class的优先级是否高于fair_sched_class的优先级，若是就返回true，否则返回false。所以调度类的优先级高低，实际就是看调度类变量的地址小的优先级高，为什么由调度类变量的地址的大小就能确定一个调度类的优先级呢？这实际跟调度类的初始化代码以及链接器角度有关。
</p>

<p>
以fair_sched_class的定义为例：
</p>
<pre class="example" id="org9a23e8a">
/*
 * All the scheduling class methods:
 */
DEFINE_SCHED_CLASS(fair) = {

	.enqueue_task		= enqueue_task_fair,
	.dequeue_task		= dequeue_task_fair,
	.yield_task		= yield_task_fair,
	.yield_to_task		= yield_to_task_fair,

      ...
};
</pre>
<pre class="example" id="org08afa7a">
/*
 * Helper to define a sched_class instance; each one is placed in a separate
 * section which is ordered by the linker script:
 *
 *   include/asm-generic/vmlinux.lds.h
 *
 * *CAREFUL* they are laid out in *REVERSE* order!!!
 *
 * Also enforce alignment on the instance, not the type, to guarantee layout.
 */
#define DEFINE_SCHED_CLASS(name) \
const struct sched_class name##_sched_class \
	__aligned(__alignof__(struct sched_class)) \
	__section("__" #name "_sched_class")
</pre>
<p>
以上定义会使得fair_sched_class这个量被放到__fair_sched_class这个section里，其它调度类也是类似的定义方式：
</p>
<pre class="example" id="org9aa9c15">
DEFINE_SCHED_CLASS(rt)
DEFINE_SCHED_CLASS(idle)
DEFINE_SCHED_CLASS(stop)
DEFINE_SCHED_CLASS(dl)
DEFINE_SCHED_CLASS(ext)
</pre>
<p>
这些调度类会被链接脚本按序放置：
</p>
<pre class="example" id="org501cfd8">
/*
 * The order of the sched class addresses are important, as they are
 * used to determine the order of the priority of each sched class in
 * relation to each other.
 */
#define SCHED_DATA				\
	STRUCT_ALIGN();				\
	__sched_class_highest = .;		\
	*(__stop_sched_class)			\
	*(__dl_sched_class)			\
	*(__rt_sched_class)			\
	*(__fair_sched_class)			\
	*(__ext_sched_class)			\
	*(__idle_sched_class)			\
	__sched_class_lowest = .;
</pre>
<p>
比如，*(__dl_sched_class)表示把所有放入__dl_sched_class段的对象，链接到当前地址，链接地址按每行依次增大，这样具有较小地址的调度类拥有更高的优先级，这样sched_class_above采用直接比较地址大小的办法来确定不同调度类的优先级就有了根据。
</p>

<p>
现在简单分析下另一个条件就是：
</p>
<pre class="example" id="org3bd004a">
rq-&gt;nr_running == rq-&gt;cfs.h_nr_queued
</pre>
<p>
条件本身是简单明晰的，就是现在通用rq运行队列里的所有正在运行的进程数量等于cfs队列里排队的进程数量，各个调度类在入队/出队都是可以操作rq::nr_running计数的，比如stop调度类出队进程时：
</p>
<pre class="example" id="orgb17423c">
dequeue_task_stop-&gt;sub_nr_running
</pre>
<p>
stop调度类入队时：
</p>
<pre class="example" id="org8f4066a">
enqueue_task_stop-&gt;add_nr_running
</pre>
<p>
对于fair CFS调度类来说也是一样，比如出队时：
</p>
<pre class="example" id="orgb06e62f">
dequeue_entities-&gt;sub_nr_running
</pre>
<p>
入队时：
</p>
<pre class="example" id="org4f9dcd3">
enqueue_task_fair-&gt;add_nr_running
</pre>
<p>
同时，如果CFS操作了rq::nr_running，那么也会同步操作cfs_rq::h_nr_queued，比如在dequeue_entities里先会对cfs_rq::h_nr_queued进行递减，才会调用sub_nr_running去递减通用rq::nr_running计数，所以如果上述的条件满足，就意味着没有其它调度类的进程被enqueue到通用rq队列里，这样就可以直接调用CFS类的pick_next_task_fair函数了。如果从CFS队列里没能选出进程，那么就可以调用pick_task_idle选一个空闲进程来运行了，当然前提是fair调度类的下一个优先级的调度类就是idle类。
</p>

<p>
restart标签下的代码，先做了下负载均衡，prev_balance会从prev所属的调度类往下的优先级去遍历调度类，然后调用这些调度类的balance函数（如果有的话），当然restart下最重要的逻辑还是从最高优先级的调度类往最低优先级的调度类遍历，去寻找一个可以运行的进程：
</p>
<pre class="example" id="org6446fca">
#define for_each_active_class(class)						\
	for_active_class_range(class, __sched_class_highest, __sched_class_lowest)
#define for_active_class_range(class, _from, _to)				\
	for (class = (_from); class != (_to); class = next_active_class(class))
/*
 * Iterate only active classes. SCX can take over all fair tasks or be
 * completely disabled. If the former, skip fair. If the latter, skip SCX.
 */
static inline const struct sched_class *next_active_class(const struct sched_class *class)
{
	class++;
#ifdef CONFIG_SCHED_CLASS_EXT
	if (scx_switched_all() &amp;&amp; class == &amp;fair_sched_class)
		class++;
	if (!scx_enabled() &amp;&amp; class == &amp;ext_sched_class)
		class++;
#endif
	return class;
}
</pre>
<p>
有了前面对于使用调度类地址反应调度优先级的原理说明，现在看这个调度类遍历宏的实现就简单多了，就不必过多介绍了。这里只是再提下sched_class::pick_next_task和sched_class::pick_task在这里的不同调用表现，前者一般就只有CFS调度类实现了，使用pick_next_task就无需调度核心框架（kernel/sched/core.c）再去调用put_prev_set_next_task函数了。
</p>

<p>
最后idle调度类的进程一定可以有一个进程运行，所以最后有个BUG。
</p>

<p>
下面开始分析pick_next_task_fair函数：
</p>
<pre class="example" id="org1fc82d3">
struct task_struct *
pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	struct sched_entity *se;
	struct task_struct *p;
	int new_tasks;

again:
	p = pick_task_fair(rq);
	if (!p)
		goto idle;
	se = &amp;p-&gt;se;

#ifdef CONFIG_FAIR_GROUP_SCHED
	if (prev-&gt;sched_class != &amp;fair_sched_class)
		goto simple;

	__put_prev_set_next_dl_server(rq, prev, p);

	/*
	 * Because of the set_next_buddy() in dequeue_task_fair() it is rather
	 * likely that a next task is from the same cgroup as the current.
	 *
	 * Therefore attempt to avoid putting and setting the entire cgroup
	 * hierarchy, only change the part that actually changes.
	 *
	 * Since we haven't yet done put_prev_entity and if the selected task
	 * is a different task than we started out with, try and touch the
	 * least amount of cfs_rqs.
	 */
	if (prev != p) {
		struct sched_entity *pse = &amp;prev-&gt;se;
		struct cfs_rq *cfs_rq;

		while (!(cfs_rq = is_same_group(se, pse))) {
			int se_depth = se-&gt;depth;
			int pse_depth = pse-&gt;depth;

			if (se_depth &lt;= pse_depth) {
				put_prev_entity(cfs_rq_of(pse), pse);
				pse = parent_entity(pse);
			}
			if (se_depth &gt;= pse_depth) {
				set_next_entity(cfs_rq_of(se), se);
				se = parent_entity(se);
			}
		}

		put_prev_entity(cfs_rq, pse);
		set_next_entity(cfs_rq, se);

		__set_next_task_fair(rq, p, true);
	}

	return p;

simple:
#endif
	put_prev_set_next_task(rq, prev, p);
	return p;

idle:
	if (!rf)
		return NULL;

	new_tasks = sched_balance_newidle(rq, rf);

	/*
	 * Because sched_balance_newidle() releases (and re-acquires) rq-&gt;lock, it is
	 * possible for any higher priority task to appear. In that case we
	 * must re-start the pick_next_entity() loop.
	 */
	if (new_tasks &lt; 0)
		return RETRY_TASK;

	if (new_tasks &gt; 0)
		goto again;

	/*
	 * rq is about to be idle, check if we need to update the
	 * lost_idle_time of clock_pelt
	 */
	update_idle_rq_clock_pelt(rq);

	return NULL;
}
</pre>
<p>
该函数主体逻辑有三方面，一是通过pick_task_fair函数去选下一个运行的进程，二是处理组调度的逻辑，三是如果没有可运行的进程，还需要调用sched_balance_newidle去拉取其它rq（cpu）上的任务。
</p>

<p>
pick_task_fair留后面分析。如果这个函数选不出进程了，也就是p为空，就会跳到idle标签，其下的逻辑就是通过sched_balance_newidle去拉取其它cpu上任务，如果能拉取成功（返回值大于0），就会跳到again通过pick_task_fair再次挑选任务运行。
</p>

<p>
在开启了组调度并且要被切换的prev进程所属的调度类是CFS时，就会有一些逻辑处理组调度，如果条件不满足就是跳到simple标签，也就是只需要简单的对prev进行put操作就行。
</p>

<p>
处理组调度的逻辑相对复杂一些，首先组调度有一个depth层级的概念，所有调度实体组成一棵调度树，叶子节点是可以运行的进程实体，而中间的层代表了其下任务的总行为。对于挑选出来即将运行p和即将切换出去的prev，如果它们不同，就会对它们循环向上降低深度，对于prev路径上的任务要进行put操作，而对于p路径上的任务要进行set操作，每次迭代期间，哪个较深，就通过parent_entity找到对应sched_entity的父调度实体，其实质也就是sched_entity::depth会减小。is_same_group会判断迭代的路径上两个sched_entity是否属于相同的组了：
</p>

<pre class="example" id="org06dc95a">
/* Do the two (enqueued) entities belong to the same group ? */
static inline struct cfs_rq *
is_same_group(struct sched_entity *se, struct sched_entity *pse)
{
	if (se-&gt;cfs_rq == pse-&gt;cfs_rq)
		return se-&gt;cfs_rq;

	return NULL;
}
</pre>
<p>
如果迭代到两个sched_entity属于同一个cfs rq时，这时put/set操作就可以停下来了，这算是一种优化，不用操作整个调度树。
</p>

<p>
前面提到了set/put操作，具体的函数实现分别就是set_next_entity和put_prev_entity，下面分析下这两个函数。首先是set_next_entity：
</p>
<pre class="example" id="orgb6011ed">
static void
set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	clear_buddies(cfs_rq, se);

	/* 'current' is not kept within the tree. */
	if (se-&gt;on_rq) {
		/*
		 * Any task has to be enqueued before it get to execute on
		 * a CPU. So account for the time it spent waiting on the
		 * runqueue.
		 */
		update_stats_wait_end_fair(cfs_rq, se);
		__dequeue_entity(cfs_rq, se);
		update_load_avg(cfs_rq, se, UPDATE_TG);

		set_protect_slice(se);
	}

	update_stats_curr_start(cfs_rq, se);
	WARN_ON_ONCE(cfs_rq-&gt;curr);
	cfs_rq-&gt;curr = se;

	/*
	 * Track our maximum slice length, if the CPU's load is at
	 * least twice that of our own weight (i.e. don't track it
	 * when there are only lesser-weight tasks around):
	 */
	if (schedstat_enabled() &amp;&amp;
	    rq_of(cfs_rq)-&gt;cfs.load.weight &gt;= 2*se-&gt;load.weight) {
		struct sched_statistics *stats;

		stats = __schedstats_from_se(se);
		__schedstat_set(stats-&gt;slice_max,
				max((u64)stats-&gt;slice_max,
				    se-&gt;sum_exec_runtime - se-&gt;prev_sum_exec_runtime));
	}

	se-&gt;prev_sum_exec_runtime = se-&gt;sum_exec_runtime;
}
</pre>
<p>
内核调度器在pick出一个进程运行前，都需要先对它进行set操作，所谓set操作，主要包括，如果进程还在等待队列上（on_rq被设置，在红黑树上就绪等待），那么需要将pick出的进程进行出队，因为一个马上就要运行的进程是不会同时又在红黑就绪树上等待的，这里可能会有点歧义，“出队”反而是代表进程要获得cpu运行了，正确的理解是，每个进程在运行前都需要enqueue到rq（红黑就绪树），在要获得cpu运行时需要dequeue，这里的enqueue/dequeue是针对红黑就绪树而说的，而不是上cpu运行来说enqueue/dequeue，一个典型的流程是：
</p>
<pre class="example" id="orgc2a7583">
唤醒 -&gt; enqueue_entity -&gt; 等待红黑树调度 -&gt; 被选中 -&gt; dequeue_entity -&gt; 上CPU
</pre>
<p>
总结起来就是，在CFS中，“出队（dequeue）”不是表示进程失去了运行权，而恰恰相反，是调度器确认它将获得运行权的标志性动作。enqueue/dequeue操作本质上是对红黑树的数据结构维护，而非进程运行与否的直接体现。
</p>

<p>
set操作还有就是更新即将运行进程的一些统计状态，最后一个比较重要的动作是设置cfs_rq::curr为当前选择出来即将运行的sched_entity，curr代表当前在cfs_rq上运行的实体，如果没有运行的进程就设置为NULL。
</p>

<p>
下面详细分析下set_next_entity的细节。clear_buddies用于清除sched_entity所在cfs_rq的buddy信息。
</p>

<p>
接下来如果sched_entity::on_rq非零，代表调度实体还在红黑就绪树上，那么这时需要通过update_stats_wait_end_fair函数更新它花在rq上的等待时间，随后__dequeue_entity出队rq，update_load_avg更新一下负载，set_protect_slice设置保护时间片，依次分析。
</p>

<pre class="example" id="orgd382f85">
static inline void
update_stats_wait_end_fair(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	struct sched_statistics *stats;
	struct task_struct *p = NULL;

	if (!schedstat_enabled())
		return;

	stats = __schedstats_from_se(se);

	/*
	 * When the sched_schedstat changes from 0 to 1, some sched se
	 * maybe already in the runqueue, the se-&gt;statistics.wait_start
	 * will be 0.So it will let the delta wrong. We need to avoid this
	 * scenario.
	 */
	if (unlikely(!schedstat_val(stats-&gt;wait_start)))
		return;

	if (entity_is_task(se))
		p = task_of(se);

	__update_stats_wait_end(rq_of(cfs_rq), p, stats);
}
</pre>

<p>
schedstat_enabled在没有开启CONFIG_SCHEDSTATS时都是0，否则就是sched_schedstats分支变量：
</p>
<pre class="example" id="org61cf6f9">
#define   schedstat_enabled()		static_branch_unlikely(&amp;sched_schedstats)
</pre>
<p>
笔者的环境开启了CONFIG_SCHEDSTATS配置，但是sched_schedstats默认是0的：
</p>
<pre class="example" id="orgd399963">
DEFINE_STATIC_KEY_FALSE(sched_schedstats);
</pre>
<p>
所以通常情况下，update_stats_wait_end_fair在schedstat_enabled判断就该返回了，也就是cat /proc/schedstat默认是看不到wait_max，wait_count以及wait_sum这些成员的。
</p>

<p>
分析sched_schedstats这个分支变量，可以通过set_schedstats这个函数开关它：
</p>
<pre class="example" id="orgdac81c4">
static void set_schedstats(bool enabled)
{
	if (enabled)
		static_branch_enable(&amp;sched_schedstats);
	else
		static_branch_disable(&amp;sched_schedstats);
}
</pre>
<p>
调用set_schedstats（也就是设置sched_schedstats）有两种手段一是通过启动命令行添加schedstats=enable，再有就是修改/proc/sys/kernel/sched_schedstats为1。
</p>

<p>
假设现在开启了sched_schedstats，那么往下就会通过__schedstats_from_se去获得sched_statistics调度统计结构体：
</p>
<pre class="example" id="orgbfc1c1e">
static inline struct sched_statistics *
__schedstats_from_se(struct sched_entity *se)
{
#ifdef CONFIG_FAIR_GROUP_SCHED
	if (!entity_is_task(se))
		return &amp;container_of(se, struct sched_entity_stats, se)-&gt;stats;
#endif
	return &amp;task_of(se)-&gt;stats;
}
</pre>
<p>
该函数的实现分两种情况，一是配置了组调度并且当前实体并不是一个实际的任务，那么sched_statistics结构体实际是内嵌在了sched_entity_stats结构体，所以可以用container_of这种结构体偏移的办法获得调度统计结构体。
</p>

<p>
这里判断一个实体是否是实际任务的办法很简单（当然是针对开启组调度CONFIG_FAIR_GROUP_SCHED而说的，没有开启这个配置时，任何调度实体sched_entity都是可上CPU运行的）：
</p>
<pre class="example" id="org3343508">
#define entity_is_task(se)	(!se-&gt;my_q)
</pre>
<p>
my_q其实就是在开启组调度时（非空），该调度实体下拥有的cfs调度队列。
</p>

<p>
没有开启组调度时，__schedstats_from_se，sched_statistics就是内嵌在task_struct里的。
</p>

<p>
拿到了sched_statistics后，就有一个防御性检查判断，那就是wait_start要非0，wait_start是可能为0的，比如按如上的使用sysctl变量的方法开启该功能时，wait_start之前没有开启时是0，这样后面再算调度实体的等待运行时间就会变得很大（或者离谱）。
</p>

<p>
再往后的代码拿了下对应调度实体的task_struct结构体。
</p>

<p>
最后是真正的更新统计动作：
</p>
<pre class="example" id="org599cfd4">
void __update_stats_wait_end(struct rq *rq, struct task_struct *p,
			     struct sched_statistics *stats)
{
	u64 delta = rq_clock(rq) - schedstat_val(stats-&gt;wait_start);

	if (p) {
		if (task_on_rq_migrating(p)) {
			/*
			 * Preserve migrating task's wait time so wait_start
			 * time stamp can be adjusted to accumulate wait time
			 * prior to migration.
			 */
			__schedstat_set(stats-&gt;wait_start, delta);

			return;
		}

		trace_sched_stat_wait(p, delta);
	}

	__schedstat_set(stats-&gt;wait_max,
			max(schedstat_val(stats-&gt;wait_max), delta));
	__schedstat_inc(stats-&gt;wait_count);
	__schedstat_add(stats-&gt;wait_sum, delta);
	__schedstat_set(stats-&gt;wait_start, 0);
}
</pre>
<p>
这个函数的逻辑是显而易见的了，用rq_clock获得的clock减去之前记录的开始等待的时间wait_start（入队时会设置），就是当前调度实体等待了多久才获得cpu，同时设置新的可能变化的wait_max，也即最大等待时间，同时wait_count自增，wait_sum累加，由于__update_stats_wait_end结束后调度实体马上会上CPU运行，所以要设置wait_start为0。
</p>

<p>
这里需要说明说明的是，task_struct::on_rq其实有三种状态，为0代表正在cpu上运行，为1代表在rq上排队，而为2代表在迁移到另一个rq的过程：
</p>
<pre class="example" id="org244bb76">
/* task_struct::on_rq states: */
#define TASK_ON_RQ_QUEUED	1
#define TASK_ON_RQ_MIGRATING	2
</pre>
<p>
task_on_rq_queued定义如下：
</p>
<pre class="example" id="org5e8db33">
static inline int task_on_rq_migrating(struct task_struct *p)
{
	return READ_ONCE(p-&gt;on_rq) == TASK_ON_RQ_MIGRATING;
}
</pre>
<p>
处理在迁移状态进程的等待时间是一个边界条件，在某个cpuA的rqA上等待了一段时间delta_A，还没有上cpuA运行时，就被迁移到了另一个cpuB时，在新的rqB上又等待了一段时间delta_B，那么总的等待时间应该是delta_A + delta_B，在__update_stats_wait_end里只是将这个delta时间给到了wait_start，也就是这时wait_start的语义其实发生了变化，不再是一个时间戳，而是一段时间长度，然后就返回了。而在__update_stats_wait_start的实现里，设置起始的开始等待时间时，会将wait_start减去这个已经等待的时间prev_wait_start（就是delta_A + delta_B）：
</p>
<pre class="example" id="org961d04c">
void __update_stats_wait_start(struct rq *rq, struct task_struct *p,
			       struct sched_statistics *stats)
{
	u64 wait_start, prev_wait_start;

	wait_start = rq_clock(rq);
	prev_wait_start = schedstat_val(stats-&gt;wait_start);

	if (p &amp;&amp; likely(wait_start &gt; prev_wait_start))
		wait_start -= prev_wait_start;

	__schedstat_set(stats-&gt;wait_start, wait_start);
}
</pre>
<p>
这样相当于把开始等待的时间往前拨了些，以达到累计等待时间的效果。对__update_stats_wait_start的调用可以是：
</p>
<pre class="example" id="org57312c8">
update_stats_enqueue_fair-&gt;update_stats_wait_start_fair-&gt;__update_stats_wait_start
</pre>
<p>
也可以是：
</p>
<pre class="example" id="org62b1da9">
put_prev_entity-&gt;update_stats_wait_start_fair-&gt;__update_stats_wait_start
</pre>
<p>
前者是调度实体入队的时候，后者是从CPU上撤下重新put到rq时，这些时机都需要记录开始等待的时间戳。
</p>

<p>
set调度实体的下一个动作是将调度实体dequeue出rq红黑树队列：
</p>
<pre class="example" id="org51cc613">
static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	rb_erase_augmented_cached(&amp;se-&gt;run_node, &amp;cfs_rq-&gt;tasks_timeline,
				  &amp;min_vruntime_cb);
	avg_vruntime_sub(cfs_rq, se);
}
</pre>
<pre class="example" id="org791ab7a">
static __always_inline void
rb_erase_augmented_cached(struct rb_node *node, struct rb_root_cached *root,
			  const struct rb_augment_callbacks *augment)
{
	if (root-&gt;rb_leftmost == node)
		root-&gt;rb_leftmost = rb_next(node);
	rb_erase_augmented(node, &amp;root-&gt;rb_root, augment);
}
</pre>
<p>
cfs_rq里的tasks_timeline成员就是大名鼎鼎的任务挂载的红黑树，其定义如下：
</p>
<pre class="example" id="org0f10e63">
struct rb_root_cached {
	struct rb_root rb_root;
	struct rb_node *rb_leftmost;
};
</pre>
<p>
rb_leftmost是缓存的整个树的所有进程里，下一个即将运行的进程，而rb_root是所有进程形成树的根节点，sched_entity::run_node用来代表调度实体往树上挂载，如果当前出队的进程就刚好是rb_leftmost的话，那么rb_leftmost需要更新，因为当前进程出队了。rb_erase_augmented里是真正的删除节点的操作，里面还会维护二叉红黑树的性质，具体的本文主题就不介绍了。由于当前调度实体出队了，一些负载要从当前的cfs_rq里扣除，这就是avg_vruntime_sub做的事情：
</p>
<pre class="example" id="org6f79126">
static void
avg_vruntime_sub(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	unsigned long weight = scale_load_down(se-&gt;load.weight);
	s64 key = entity_key(cfs_rq, se);

	cfs_rq-&gt;avg_vruntime -= key * weight;
	cfs_rq-&gt;avg_load -= weight;
}
</pre>
<p>
进程的weight一般通过set_load_weight去设置。
</p>

<p>
下一个动作是通过update_load_avg进行负载更新，进程的负载计算本篇主题不打算详细介绍，另开PELT算法主题单独介绍，总的来说该函数会将负载按周期进行衰减，这里的周期是1024us，也就是约为1ms，当前周期对负载的共享就是L，过去1ms内是L * y1，过去第2ms内是L * y^2，依次类推，其中y ^32 = 0.5。
</p>

<p>
update_stats_curr_start更新了现在调度实体开始运行的时间戳：
</p>
<pre class="example" id="orgba58715">
/*
 * We are picking a new current task - update its stats:
 */
static inline void
update_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	/*
	 * We are starting a new run period:
	 */
	se-&gt;exec_start = rq_clock_task(rq_of(cfs_rq));
}
</pre>
<p>
cfs_rq当前运行的实体curr也要改成当前的se。
</p>

<p>
最后的一段代码是记录下当前se的最大运行slice时间片，用当前的sum_exec_runtime减去上次的这个累计执行时间，并和之前的取大者。不过这个更新是有条件的，那就是当前cfs_rq的负载要较大（大于sed两倍）。这样set动作就介绍完了。
</p>

<p>
下面是介绍put动作：
</p>
<pre class="example" id="orgca5b024">
static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)
{
	/*
	 * If still on the runqueue then deactivate_task()
	 * was not called and update_curr() has to be done:
	 */
	if (prev-&gt;on_rq)
		update_curr(cfs_rq);

	/* throttle cfs_rqs exceeding runtime */
	check_cfs_rq_runtime(cfs_rq);

	if (prev-&gt;on_rq) {
		update_stats_wait_start_fair(cfs_rq, prev);
		/* Put 'current' back into the tree. */
		__enqueue_entity(cfs_rq, prev);
		/* in !on_rq case, update occurred at dequeue */
		update_load_avg(cfs_rq, prev, 0);
	}
	WARN_ON_ONCE(cfs_rq-&gt;curr != prev);
	cfs_rq-&gt;curr = NULL;
}
</pre>
<p>
如果prev在rq上，就需要更新下当前sched_entity的一些运行时信息，最重要的信息当属sched_entity::vruntime（较新的内核使用新的调度算法，sched_entity::deadline替代了vruntime的功能），因为它（较新的内核是sched_entity::deadline）会用来确定插入进程红黑树的位置，而插入的位置又决定调度哪个进程上CPU运行：
</p>
<pre class="example" id="org7c35a3e">
/*
 * Update the current task's runtime statistics.
 */
static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq-&gt;curr;
	struct rq *rq = rq_of(cfs_rq);
	s64 delta_exec;
	bool resched;

	if (unlikely(!curr))
		return;

	delta_exec = update_curr_se(rq, curr);
	if (unlikely(delta_exec &lt;= 0))
		return;

	curr-&gt;vruntime += calc_delta_fair(delta_exec, curr);
	resched = update_deadline(cfs_rq, curr);
	update_min_vruntime(cfs_rq);

	if (entity_is_task(curr)) {
		struct task_struct *p = task_of(curr);

		update_curr_task(p, delta_exec);

		/*
		 * If the fair_server is active, we need to account for the
		 * fair_server time whether or not the task is running on
		 * behalf of fair_server or not:
		 *  - If the task is running on behalf of fair_server, we need
		 *    to limit its time based on the assigned runtime.
		 *  - Fair task that runs outside of fair_server should account
		 *    against fair_server such that it can account for this time
		 *    and possibly avoid running this period.
		 */
		if (dl_server_active(&amp;rq-&gt;fair_server))
			dl_server_update(&amp;rq-&gt;fair_server, delta_exec);
	}

	account_cfs_rq_runtime(cfs_rq, delta_exec);

	if (cfs_rq-&gt;nr_queued == 1)
		return;

	if (resched || did_preempt_short(cfs_rq, curr)) {
		resched_curr_lazy(rq);
		clear_buddies(cfs_rq, curr);
	}
}
</pre>
<p>
先通过update_curr_se算出delta_exec：
</p>
<pre class="example" id="org47c45c7">
static s64 update_curr_se(struct rq *rq, struct sched_entity *curr)
{
	u64 now = rq_clock_task(rq);
	s64 delta_exec;

	delta_exec = now - curr-&gt;exec_start;
	if (unlikely(delta_exec &lt;= 0))
		return delta_exec;

	curr-&gt;exec_start = now;
	curr-&gt;sum_exec_runtime += delta_exec;

	if (schedstat_enabled()) {
		struct sched_statistics *stats;

		stats = __schedstats_from_se(curr);
		__schedstat_set(stats-&gt;exec_max,
				max(delta_exec, stats-&gt;exec_max));
	}

	return delta_exec;
}
</pre>
<p>
delta_exec开始就是进程运行了多少时间，同时这个函数还更新了exec_start为新的现在的时间，累加了进程总运行时间sum_exec_runtime，进程运行的最大时间长度exec_max也可能会被更新。
</p>

<p>
接下来的calc_delta_fair里的逻辑是体现哪个进程优先调度的关键逻辑：
</p>
<pre class="example" id="orgee6b3ae">
/*
 * delta /= w
 */
static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
{
	if (unlikely(se-&gt;load.weight != NICE_0_LOAD))
		delta = __calc_delta(delta, NICE_0_LOAD, &amp;se-&gt;load);

	return delta;
}
</pre>
<pre class="example" id="org3fa5e4a">
/*
 * delta_exec * weight / lw.weight
 *   OR
 * (delta_exec * (weight * lw-&gt;inv_weight)) &gt;&gt; WMULT_SHIFT
 *
 * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case
 * we're guaranteed shift stays positive because inv_weight is guaranteed to
 * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift &gt;= 22.
 *
 * Or, weight =&lt; lw.weight (because lw.weight is the runqueue weight), thus
 * weight/lw.weight &lt;= 1, and therefore our shift will also be positive.
 */
static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)
{
	u64 fact = scale_load_down(weight);
	u32 fact_hi = (u32)(fact &gt;&gt; 32);
	int shift = WMULT_SHIFT;
	int fs;

	__update_inv_weight(lw);

	if (unlikely(fact_hi)) {
		fs = fls(fact_hi);
		shift -= fs;
		fact &gt;&gt;= fs;
	}

	fact = mul_u32_u32(fact, lw-&gt;inv_weight);

	fact_hi = (u32)(fact &gt;&gt; 32);
	if (fact_hi) {
		fs = fls(fact_hi);
		shift -= fs;
		fact &gt;&gt;= fs;
	}

	return mul_u64_u32_shr(delta_exec, fact, shift);
}
</pre>
<p>
从这些逻辑可以看到，是把delta_exec进行了缩放，缩放的比例就是NICE_0_LOAD/weight_of_se，而sched_entity::weight经由函数set_load_weight设置：
</p>
<pre class="example" id="org5033dec">
void set_load_weight(struct task_struct *p, bool update_load)
{
	int prio = p-&gt;static_prio - MAX_RT_PRIO;
	struct load_weight lw;

	if (task_has_idle_policy(p)) {
		lw.weight = scale_load(WEIGHT_IDLEPRIO);
		lw.inv_weight = WMULT_IDLEPRIO;
	} else {
		lw.weight = scale_load(sched_prio_to_weight[prio]);
		lw.inv_weight = sched_prio_to_wmult[prio];
	}

	/*
	 * SCHED_OTHER tasks have to update their load when changing their
	 * weight
	 */
	if (update_load &amp;&amp; p-&gt;sched_class-&gt;reweight_task)
		p-&gt;sched_class-&gt;reweight_task(task_rq(p), p, &amp;lw);
	else
		p-&gt;se.load = lw;
}
</pre>
<p>
这里比较关键的就是有个sched_prio_to_weight数组：
</p>
<pre class="example" id="orga5b0a63">
/*
 * Nice levels are multiplicative, with a gentle 10% change for every
 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
 * nice 1, it will get ~10% less CPU time than another CPU-bound task
 * that remained on nice 0.
 *
 * The "10% effect" is relative and cumulative: from _any_ nice level,
 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
 * If a task goes up by ~10% and another task goes down by ~10% then
 * the relative distance between them is ~25%.)
 */
const int sched_prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};
</pre>
<p>
该数组是按nice值作为优先级填充的，相邻的nice值差距大约25%的CPU使用率，nice为0定义为1024。与之对应的是2^32 / weight的另一个数组sched_prio_to_wmult，这个值是为后面除以weight提前算的，因为对weight做除，效率较低，可以直接乘以这个数组里的值再右移2^32位，抵消了提前乘的2^32次方：
</p>
<pre class="example" id="orgfb0ceb6">
/*
 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, pre-calculated.
 *
 * In cases where the weight does not change often, we can use the
 * pre-calculated inverse to speed up arithmetics by turning divisions
 * into multiplications:
 */
const u32 sched_prio_to_wmult[40] = {
 /* -20 */     48388,     59856,     76040,     92818,    118348,
 /* -15 */    147320,    184698,    229616,    287308,    360437,
 /* -10 */    449829,    563644,    704093,    875809,   1099582,
 /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
 /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
 /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
 /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
 /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};
</pre>
<p>
具体的__calc_delta里的计算方式会有一些优化，就不详细分析了。这样__calc_delta最后返回值的效果就是，对于权重越大的（nice值越低），NICE_0_LOAD / weight这个比率就越小接近0，这样会把delta_exec缩放的越小，那么update_curr里的语句：
</p>
<pre class="example" id="org3de16c0">
curr-&gt;vruntime += calc_delta_fair(delta_exec, curr);
</pre>
<p>
往vruntime里累加的值就越小，而往进程红黑树里插入进程时比较的key值就是这里的vruntime（较新的调度算法是sched_entity::deadline，但原理类似），越小的vruntime越容易出现在进程调度红黑树的左半部分而选择被调度运行。
</p>
</div>
</div>

<div id="outline-container-org6da77ff" class="outline-2">
<h2 id="org6da77ff"><span class="section-number-2">3.</span> context_switch</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Cauchy(pqy7172@gmail.com)</p>
<p class="date">Created: 2025-05-11 Sun 22:00</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
