<!DOCTYPE html>
<html lang="cn">
<head>
<!-- 2025-03-10 Mon 22:23 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>进程调度</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Cauchy(pqy7172@gmail.com)">
<link rel="stylesheet" href="../../org-manual.css" type="text/css">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">进程调度</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4d3ee12">1. __schedule</a></li>
<li><a href="#org14ac33d">2. pick_next_task</a></li>
<li><a href="#org3ed9357">3. context_switch</a></li>
</ul>
</div>
</div>
<p>
__schedule是进程调度的核心函数，它先调用pick_next_task函数算出需要运行哪个进程，然后调用context_switch函数完成切换。
</p>

<p>
驱动调度器并进入__schedule这个核心调度函数主要有以下一些方式：
</p>

<ul class="org-ul">
<li>显式阻塞，例如互斥锁（mutex）、信号量（semaphore）、等待队列（waitqueue）等，任务主动睡眠进入等待状态，比如进程调用了mutex_lock、down或wait_event等函数可能会导致自己进入等待状态，需要调度器选择下一个可运行的任务。</li>
<li>定时器的中断处理程序sched_tick可以设置TIF_NEED_RESCHED标志来驱动任务间的抢占调度，该标志会在中断返回和用户态返回路径上被检查。</li>
<li>唤醒任务时并不会直接导致schedule被调用，它只是将任务添加到运行队列（run-queue）中。但如果新加入的任务优先级更高，导致抢占，那么唤醒代码会设置TIF_NEED_RESCHED，然后schedule将在最近的时机被调用，这种时机又分内核是否开启抢占而有所不同：
<ul class="org-ul">
<li>如果内核是可抢占的（CONFIG_PREEMPTION=y）：在系统调用或异常（syscall/exception）上下文中，会在最外层preempt_enable时触发抢占调度，这可能会在wake_up的spin_unlock之后立即发生。在中断（IRQ）上下文中，会在从中断处理程序返回到可抢占上下文时进行调度。</li>
<li>如果内核是不可抢占的（CONFIG_PREEMPTION未启用），那么调度将在以下情况下发生，1显式主动调用cond_resched，2显式主动调用schedule，3从系统调用或异常返回到用户态，4从中断处理程序返回到用户态。</li>
</ul></li>
</ul>

<p>
调用__schedule时必须禁用抢占（preemption disabled），但其父函数可能会处理抢占的问题，比如__schedule__loop就会禁用抢占。
</p>

<p>
__schedule函数的参数sched_mode指明了以何种模式进入的调度器，它的取值可以有：
</p>
<pre class="example" id="org014288f">
/*
 * Constants for the sched_mode argument of __schedule().
 *
 * The mode argument allows RT enabled kernels to differentiate a
 * preemption from blocking on an 'sleeping' spin/rwlock.
 */
#define SM_IDLE			(-1)
#define SM_NONE			0
#define SM_PREEMPT		1
#define SM_RTLOCK_WAIT		2
</pre>
<p>
内核调度器在调度空闲任务时会使用SM_IDLE模式，表示当前没有可运行的普通任务，CPU可能会进入低功耗模式。
</p>

<p>
SM_NONE是普通调度模式，不涉及特殊的抢占或锁等待情况。这是最常见的调度模式，表示当前任务只是正常调度，而不是因为抢占或等待锁。
</p>

<p>
SM_PREEMPT表示任务被抢占（preempted）。当更高优先级的任务就绪时，内核可以抢占当前任务。比如普通任务被抢占式内核（Preemptible Kernel）或实时内核抢占时使用此模式。软实时（SCHED_RR）或硬实时（SCHED_FIFO）任务可能会触发此模式。
</p>

<p>
SM_RTLOCK_WAIT表示任务正在等待RT（实时）锁。实时内核（RT内核）提供了一种机制，使spinlock和rwlock可以在高优先级任务上睡眠（普通内核的自旋锁是不会睡眠的）。当任务因为获取实时锁（RT lock）而阻塞时，会使用这个模式。
</p>

<p>
总的来说这些调度模式主要用于区分不同的调度情况，特别是在抢占式调度和RT内核下，普通任务切换使用SM_NONE，表示任务因为时间片耗尽或显式调度而切换。抢占（Preemption）使用SM_PREEMPT，表示任务因为更高优先级任务到来而被抢占。空闲调度使用SM_IDLE，表示CPU进入空闲模式。实时锁等待使用SM_RTLOCK_WAIT，表示任务在RT互斥锁（如rtmutex）上睡眠等待。
</p>

<p>
比如preempt_schedule_common函数调用_schedule时就指明了参数SM_PREEMPT，表明本次进入调度器是因为抢占。
</p>

<div id="outline-container-org4d3ee12" class="outline-2">
<h2 id="org4d3ee12"><span class="section-number-2">1.</span> __schedule</h2>
<div class="outline-text-2" id="text-1">
<p>
以下是__schedule的实现：
</p>
<pre class="example" id="org460521d">
static void __sched notrace __schedule(int sched_mode)
{
	struct task_struct *prev, *next;
	/*
	 * On PREEMPT_RT kernel, SM_RTLOCK_WAIT is noted
	 * as a preemption by schedule_debug() and RCU.
	 */
	bool preempt = sched_mode &gt; SM_NONE;
	unsigned long *switch_count;
	unsigned long prev_state;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq-&gt;curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()-&gt;signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up():
	 *
	 * __set_current_state(@state)		signal_wake_up()
	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING)
	 *					  wake_up_state(p, state)
	 *   LOCK rq-&gt;lock			    LOCK p-&gt;pi_state
	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock()
	 *     if (signal_pending_state())	    if (p-&gt;state &amp; @state)
	 *
	 * Also, the membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq-&gt;curr; this
	 * barrier matches a full barrier in the proximity of the membarrier
	 * system call exit.
	 */
	rq_lock(rq, &amp;rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq-&gt;clock_update_flags &lt;&lt;= 1;
	update_rq_clock(rq);
	rq-&gt;clock_update_flags = RQCF_UPDATED;

	switch_count = &amp;prev-&gt;nivcsw;

	/* Task state changes only considers SM_PREEMPT as preemption */
	preempt = sched_mode == SM_PREEMPT;

	/*
	 * We must load prev-&gt;state once (task_struct::state is volatile), such
	 * that we form a control dependency vs deactivate_task() below.
	 */
	prev_state = READ_ONCE(prev-&gt;__state);
	if (sched_mode == SM_IDLE) {
		/* SCX must consult the BPF scheduler to tell if rq is empty */
		if (!rq-&gt;nr_running &amp;&amp; !scx_enabled()) {
			next = prev;
			goto picked;
		}
	} else if (!preempt &amp;&amp; prev_state) {
		try_to_block_task(rq, prev, prev_state);
		switch_count = &amp;prev-&gt;nvcsw;
	}

	next = pick_next_task(rq, prev, &amp;rf);
	rq_set_donor(rq, next);
picked:
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();
#ifdef CONFIG_SCHED_DEBUG
	rq-&gt;last_seen_need_resched_ns = 0;
#endif

	if (likely(prev != next)) {
		rq-&gt;nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq-&gt;curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq-&gt;curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq-&gt;curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC,
		 *   RISC-V.  switch_mm() relies on membarrier_arch_switch_mm()
		 *   on PowerPC and on RISC-V.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 *
		 * The barrier matches a full barrier in the proximity of
		 * the membarrier system call entry.
		 *
		 * On RISC-V, this barrier pairing is also needed for the
		 * SYNC_CORE command when switching between processes, cf.
		 * the inline comments in membarrier_arch_switch_mm().
		 */
		++*switch_count;

		migrate_disable_switch(rq, prev);
		psi_account_irqtime(rq, prev, next);
		psi_sched_switch(prev, next, !task_on_rq_queued(prev) ||
					     prev-&gt;se.sched_delayed);

		trace_sched_switch(preempt, prev, next, prev_state);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &amp;rf);
	} else {
		rq_unpin_lock(rq, &amp;rf);
		__balance_callbacks(rq);
		raw_spin_rq_unlock_irq(rq);
	}
}
</pre>
<p>
该函数本身比较简单，每个cpu都有一关联的runqueues，首先是通过smp_processor_id获取当前运行cpu的编号：
</p>
<pre class="example" id="org0f13087">
# define smp_processor_id() __smp_processor_id()
</pre>
<p>
不同架构有不同的__smp_processor_id实现，对于x86架构来说，每个cpu都维护有一个pcpu_hot结构体，里面有cpu_number成员记录了当前运行的cpu号，cpu_number在初始化的时候通过start_kernel-&gt;setup_per_cpu_areas去设置：
</p>
<pre class="example" id="org4e450d1">
for_each_possible_cpu(cpu) {
        ...
        per_cpu(pcpu_hot.cpu_number, cpu) = cpu;
        ...
}
</pre>
<p>
而对于其它架构比如arm64，则是在当前运行线程中的一个成员进行记录：
</p>
<pre class="example" id="org7424521">
#define raw_smp_processor_id() (current_thread_info()-&gt;cpu)
</pre>
<p>
有了cpu号，就可以通过cpu_rq获得对应当前运行cpu的rq运行队列了：
</p>
<pre class="example" id="org97e1555">
DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
#define cpu_rq(cpu)		(&amp;per_cpu(runqueues, (cpu)))
</pre>
<p>
通过DECLARE就为每个cpu都开辟了rq的空间，这样per_cpu就可以依据cpu号取到对应的rq运行队列。
</p>
</div>
</div>
<div id="outline-container-org14ac33d" class="outline-2">
<h2 id="org14ac33d"><span class="section-number-2">2.</span> pick_next_task</h2>
</div>
<div id="outline-container-org3ed9357" class="outline-2">
<h2 id="org3ed9357"><span class="section-number-2">3.</span> context_switch</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Cauchy(pqy7172@gmail.com)</p>
<p class="date">Created: 2025-03-10 Mon 22:23</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
