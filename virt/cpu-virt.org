#+TITLE: CPU计算虚拟化：KVM与QEMU
#+AUTHOR: Cauchy(pqy7172@gmail.com)
#+OPTIONS: ^:nil
#+EMAIL: pqy7172@gmail.com
#+HTML_HEAD: <link rel="stylesheet" href="../org-manual.css" type="text/css">
本文主要分析下KVM/QEMU对CPU的虚拟化。KVM/QEMU还完成了许多的功能，比如内存虚拟化，中断虚拟化等，但这些功能不是本文的主题。

本文分五个方面来分析CPU的虚拟化，依次展开。

* 虚拟机的创建
QEMU在初始化阶段，通过如下调用链到kvm_init函数：
: main->qemu_init->configure_accelerators->qemu_opts_foreach->do_configure_accelerator->accel_init_machine->kvm_init
在kvm_init函数中有如下代码：
#+begin_src c
  KVMState *s;
  s = KVM_STATE(ms->accelerator);
  s->fd = qemu_open_old("/dev/kvm", O_RDWR);
#+end_src
可以看到，这里打开了/dev/kvm文件，并且将返回的文件描述符保存在了KVMState的fd成员里。将来通过这个fd可以调用ioctl，内核的kvm模块其功能就是通过/dev/kvm设备文件导出的。关于这点后面再详细分析。现在还需要继续详细分析这几行QEMU代码。

首先是类型为KVMState的变量s的来源。代码中出现了KVM_STATE宏，QEMU代码中到处充斥中这种大写的宏，其定义都是类似的方式，针对KVM_STATE来说如下代码定义了它：
#+begin_src c
  DECLARE_INSTANCE_CHECKER(KVMState, KVM_STATE,
                           TYPE_KVM_ACCEL)
#+end_src
而DECLARE_INSTANCE_CHECKER定义在include/qom/object.h中。

简单介绍下qom，qom是QEMU Object Model的简写，是一种用于构建和管理QEMU设备的对象模型。QOM允许以一种层次化和组合性的方式构建虚拟设备，使得设备的模拟和管理更加灵活和可扩展。

QOM为设备提供了一种统一的方式来定义和组织属性、方法和信号。每个设备都是一个QOM实例，具有一组属性（例如，CPU的时钟频率、内存大小等）、方法（例如，设备初始化、读取寄存器等）以及可能的信号（事件通知）。同时也提供了更好的代码复用性。

在QEMU源代码中，有许多与QOM相关的代码，包括设备的定义、实例化、属性、方法和信号的设置等。

说回DECLARE_INSTANCE_CHECKER，其定义为：
#+begin_src c
  #define DECLARE_INSTANCE_CHECKER(InstanceType, OBJ_NAME, TYPENAME)	\
          static inline G_GNUC_UNUSED InstanceType *			\
          OBJ_NAME(const void *obj)					\
          { return OBJECT_CHECK(InstanceType, obj, TYPENAME); }
#+end_src
可以看到，OBJ_NAME作为传进来的参数，被定义为只有一行return的函数，当然这个函数很短，肯定是要内联的，但这不影响在分析代码时，就把它当作函数看待。继续看OBJECT_CHECK：
#+begin_src c
  #define OBJECT_CHECK(type, obj, name)					\
          ((type *)object_dynamic_cast_assert(OBJECT(obj), (name),	\
                                              __FILE__, __LINE__, __func__))
#+end_src
看到这里就明白了，其实就是个强转，将传进来的void *指针转换为指向KVMState类型的指针。ms->accelerator的类型是AccelState，那么想必KVMState的第一个成员就是AccelState类型的成员了，这样它们才能强转。相当于（ms）拥有一个AccelState类型的指针时，而该AccelState来自KVMState，那么可以直接强转为KVMState类型，就可以引用KVMState里除开AccelState外的其它成员了。这种强转其实就是个继承关系了，在各种软件项目里屡见不鲜，不论它们以何种方式（外衣）出现。

那么现在就是分析ms->accelerator的出处了。在accel_init_machine里有：
#+begin_src c
  int accel_init_machine(AccelState *accel, MachineState *ms)
  {
          ms->accelerator = accel;
  }
#+end_src
还需要再往上看父函数，在do_configure_accelerator中：
#+begin_src c
  static int do_configure_accelerator(void *opaque, QemuOpts *opts, Error **errp)
  {
          const char *acc = qemu_opt_get(opts, "accel");
          AccelClass *ac = accel_find(acc);
          AccelState *accel;
          accel = ACCEL(object_new_with_class(OBJECT_CLASS(ac)));
          ret = accel_init_machine(accel, current_machine);
  }
#+end_src
看到这里就明晰了，qemu_opt_get就是根据传入的参数去找accelerator的字符串名字，比如在启动qemu时传入了-enable-kvm，那么这里的acc就是指向串“kvm”了。而accel_find往下一路调用，最后会来到glib提供的g_hash_table_lookup函数，也就是说，包括accelerator在内的很多对象，都会事先存在hash表里，然后后面要用时通过“kvm”这样的key去找，当然真正的key串可能是组合了一些其它的字符，比如kvm-accel，见accel_find->ACCEL_CLASS_NAME。

顺便提下，current_machine也是先在初始化阶段的qemu_create_machine里创建出来：
#+begin_src c
current_machine = MACHINE(object_new_with_class(OBJECT_CLASS(machine_class)));
#+end_src

这里以kvm accelerator这种type类型的对象注册为例，来分析下一个对象的注册到hash表的流程，有此一例，其它对象的注册类似。在accel/kvm/kvm-all.c中，有如下代码：
#+begin_src c
  static const TypeInfo kvm_accel_type = {
          .name = TYPE_KVM_ACCEL,
          .parent = TYPE_ACCEL,
          .instance_init = kvm_accel_instance_init,
          .class_init = kvm_accel_class_init,
          .instance_size = sizeof(KVMState),
  };
  static void kvm_type_init(void)
  {
          type_register_static(&kvm_accel_type);
  }
#+end_src
从type_register_static一路往下的话，就会看到g_hash_table_insert，这个就和前面通过
g_hash_table_lookup去寻找加速器对应起来了，只有这里insert，后面才能lookup。现在还有唯一一
个问题，那就是kvm_type_init这个函数何时执行，qemu项目的代码里并没有找到直接调用这个函数的
地方。秘密就在这行代码：
#+begin_src c
type_init(kvm_type_init);
#+end_src
type_init定义如下：
#+begin_src c
  #define type_init(function) module_init(function, MODULE_INIT_QOM)
#+end_src
#+begin_src c
#define module_init(function, type)					      \
static void __attribute__((constructor)) do_qemu_init_ ## function(void)    \
{								              \
    register_module_init(function, type);                               \ 
}
#+end_src
可以看到这里用了constructor修饰，被这个gcc属性修饰的函数，会先于main函数的执行，而register_module_init关键的就下面几行：
#+begin_src c
  ModuleEntry *e;
  e->init = fn;
  QTAILQ_INSERT_TAIL(l, e, node);
#+end_src
可以看到也就是把传进来的kvm_type_init给放到了e->init里，可以理解为先于main执行的这段代码，主要是个注册作用，而真正调用这个动作还是在main里做的：
: main->qemu_init->qemu_init_subsystems->module_call_init->kvm_type_init

上述流程当然是先于configure_accelerator里通过accel_find去寻找加速器的，也就是main调用的qemu_init函数里，是先调用qemu_init_subsystems而后调用configure_accelerator的。

继续看给accel真正分配空间就是在object_new_with_with_class里了：
: object_new_with_class->object_new_with_type->g_malloc
  
这里又出现了大写的ACCEL宏，可以想见其作用无非就是强转指针类型了：
#+begin_src c
  #define ACCEL(obj)					\
          OBJECT_CHECK(AccelState, (obj), TYPE_ACCEL)
#+end_src
到现在就有了一个KVMState，现在回过头来给出KVMState的一些作用，可以理解为内核实现了KVM模块，那么它在QEMU这样的用户程序里有一个代表就是KVMState，其中的fd成员就保存了打开/dev/kvm时返回的fd。每次运行一个QEMU程序，就会申请一个这个结构体。

回到kvm_init函数，再往下有如下重要的代码：
#+begin_src c
  do{
          ret = kvm_ioctl(s, KVM_CREATE_VM, type);
   } while (ret == -EINTR);
  s->vmfd = ret;
#+end_src
#+begin_src c
  int kvm_ioctl(KVMState *s, int type, ...)
  {
          ret = ioctl(s->fd, type, arg);
          return ret;
  }
#+end_src
可以看到这里就使用了前面打开/dev/kvm的fd来调用ioctl，这个ioctl支持KVM_CREATE_VM这样的命令，在内核里的实现是kvm_dev_ioctl。并且将返回的ret文件描述符保存到了KVMState的vmfd成员，将来又可以在这个fd上调用它的ioctl，比如创建vcpu，在内核里对应这个vmfd的ioctl函数实现是kvm_vm_ioctl。关于这点后面还会分析。

*可以总结下，通过调用qemu_open_old函数，由/dev/kvm文件得到的fd，表示了一个KVM模块功能的合集，而通过KVM_CREATE_VM命令在/dev/kvm上的fd调用ioctl得到的vmfd，表示了一台虚拟机。*

在分析内核侧前，想特别的提下，QEMU作为一个用户程序，不论其代码怎么写，其起始入口函数都是softmmu/main.c:main，从这里入口会运行很多复杂的流程。而分析代码从这个函数起始，也算是扭住了千头万绪的线头。一些流程上先后顺序的确定也会很清晰。

现在可以看下内核KVM侧关于虚拟机创建的实现了。首先是初始化阶段/dev/kvm设备文件的注册，只有注册好了这个文件，用户程序才能open它，并在这上面使用ioctl函数。

内核的虚拟化实现是作为一个模块而存在，amd的svm，和intel的vmx实现不一样。但是内核抽出了公共部分在virt/kvm下。具体来说，针对/dev/kvm的初始化注册，由kvm_init函数里调用misc_register来实现。而对kvm_init的调用，对于intel来说就是arch/x86/kvm/vmx/vmx.c：
#+begin_src c
  module_init(vmx_init);
#+end_src
#+begin_src c
  static int __init vmx_init(void)
  {
          r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
                       __alignof__(struct vcpu_vmx), THIS_MODULE);
  }
#+end_src
#+begin_src c
  int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
               struct module *module)
  {
          r = misc_register(&kvm_dev);
  }
#+end_src
这里的kvm_dev定义为：
#+begin_src c
  static struct file_operations kvm_chardev_ops = {
          .unlocked_ioctl = kvm_dev_ioctl,
          .llseek         = noop_llseek,
          KVM_COMPAT(kvm_dev_ioctl),
  };
  
  static struct miscdevice kvm_dev = {
          KVM_MINOR,
          "kvm",
          &kvm_chardev_ops,
  };
#+end_src
可以看到，/dev/kvm文件关联的fop就是kvm_chardev_ops，对/dev/kvm使用ioctl时，就会来到kvm_dev_ioctl函数。现在主要关心kvm_ioctl以KVM_CREATE_VM调用ioctl时的内核代码，在这个命令下，内核会进入kvm_dev_ioctl_create_vm函数。该函数比较关键的流程如下：
#+begin_src c
  int r;
  struct kvm *kvm;
  kvm = kvm_create_vm(type);
  r = get_unused_fd_flags(O_CLOEXEC);
  file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
  fd_install(r, file);
  return r;
#+end_src
先是创建一个kvm结构体，一台vm都有一个kvm结构体，而kvm结构体里有内核对VCPU的表示比如kvm_cpu结构体，所有VCPU都在vcpus数组里，也有mm_struct结构体，vm所用的虚拟内存就是用户进程的虚拟地址空间，当然还有其它很多信息。

然后通过get_unused_fd_flags得到一个文件描述符r，最后返回的是这个文件描述符。随后调用anon_inode_getfile分配一个匿名的inode，file->private_data就是kvm，将来通过这个file（或fd）都可以找到kvm结构体。当然，fd和file要关联到当前进程的打开文件描述符表current->files里，后面通过fdget函数得到fd关联的file。

前面已经总结强调过了，get_unused_fd_flags返回的vmfd就是代表了一台虚拟机，Linux一切皆文件，针对这个文件（虚拟机），其操作的函数集就是kvm_vm_fops，这个fops里最关键的就是kvm_vm_ioctl函数了。比如针对一台虚拟机可以创建vcpu，这些事情都是后面小节的主题了。

总之到目前，总算是有了一台虚拟机，对比真实硬件机器来说，可以理解为主板以及上面的插槽这些都做好了，就等后面插上CPU（创建VCPU），插上需要的外设等。
* QEMU CPU创建
QEMU里虚拟CPU的创建主要是通过kvm_start_vcpu_thread，新起一个线程去创建的，并且这个线程就代表了VCPU去运行。以下是kvm_start_vcpu_thread被如下调用链调用：
: main->qemu_init->qmp_x_exit_preconfig->qemu_init_board->machine_run_board_init->pc_init1->x86_cpus_init->x86_cpu_new->qdev_realize->object_property_set_bool->object_property_set_qobject->object_property_set->
: property_set_bool->device_set_realized->x86_cpu_realizefn->qemu_init_vcpu->kvm_start_vcpu_thread
以上其实就是主板的初始化流程，就这个调用链而言，最后的函数才关心的是CPU具现化。以下分析这个调用链本身是怎么被调用起来的，分析这点更多的是涉及到QOM，也就是QEMU代码的组织，而关于在QEMU代码里VCPU创建本身在本节偏后部分会介绍到。

首先看machine_run_board_init是如何调用起pc_init1的，在machine_run_board_init函数里：
#+begin_src c
  void machine_run_board_init(MachineState *machine, const char *mem_path, Error **errp)
  {
          MachineClass *machine_class = MACHINE_GET_CLASS(machine);
          machine_class->init(machine);
  }
#+end_src
这里init函数在我的硬件平台就是pc_init1。这个回调是怎么设置的呢？

QEMU里x86平台的虚拟主板分为两类，一是i440fx，二是q35。i440fx是一个用于模拟Intel 440fx芯片组的虚拟平台。Intel i440fx芯片组是20世纪90年代早期的一个常见的PC主板芯片组，用于支持Intel的Pentium和Pentium Pro处理器。在虚拟化环境中，i440fx主板模拟了这个旧型号的主板，以便运行旧的操作系统或应用程序，或者为测试和开发目的。而q35模拟了更现代的主板和硬件特性，以便更好地支持现代操作系统和应用程序。

不论是i440fx还是q35，都是通过DEFINE_PC_MACHINE宏来注册一个具体的pc machine实例。而DEFINE_PC_MACHINE里就会设置好init函数，init本身是pc_init_##suffix，但这个函数里面会调用pc_init1。对于i440fx来说，就是DEFINE_I440FX_MACHINE里调用DEFINE_PC_MACHINE。

继续往后看object_property_set函数，在object_property_set里有个set回调，它主要是看一个对象（这里是CPU）是否具有realized属性，
若有的话， *那么object_property_set里怎么就能按如下方式调用set函数呢？*
#+begin_src c
  bool object_property_set(Object *obj, const char *name, Visitor *v,
                           Error **errp)
  {
          ObjectProperty *prop = object_property_find_err(obj, name, errp);
          prop->set(obj, v, name, prop->opaque, errp);
  }
#+end_src
这里set函数其实就是property_set_bool。
先列一个调用链：
: main->qemu_init->qemu_create_machine->select_machine->object_class_get_list->object_class_foreach->g_hash_table_foreach->object_class_foreach_tramp->type_initialize(递归三次)->device_class_init->object_class_property_add_bool
这里先分析下这个调用链是在干什么，再分析最后的object_class_property_add_bool。简要的说，这个调用链是要对加入type_table哈希表中的所有类型进行初始化，具体的说，包括设置（ *不是调用*
）具现化的回调函数，比如device_set_realized，后面真正具现化的时候再调用这样的回调函数。

到这里可以总结下QEMU代码的流程了，主要是以下几个阶段：
1. 注册。
2. 初始化。
3. 具现化。
4. 运行时。
记住这几个阶段，以后在处理QEMU+KVM的具体问题或者继续深挖代码时，能时刻清晰自己处于哪个阶段，而不只是具体问题的“指哪打哪”很局部，还多了一点全局观念或角度。

上面object_class_property_add_bool的调用链包括后面分析的object_class_property_add_bool都是归结于初始化阶段。而本节一开始分析CPU的创建过程其实是具体设备的具现化阶段了。下面再介绍一下注册阶段。以设备类的TypeInfo，device_type_info量为例来分析这个注册过程。其实关于这个type init的过程上节对kvm_accel_type注册的分析已经涉及过了，已经不是新鲜玩意了，首先有：
#+begin_src c
type_init(qdev_register_types)
#+end_src
这个type_init怎么运行起来上节已经介绍了，这里只是从qdev_register_types一路往下，看看type_table_add函数：
#+begin_src c
  static void type_table_add(TypeImpl *ti)
  {
          g_hash_table_insert(type_table_get(), (void *)ti->name, ti);
  }
  static GHashTable *type_table_get(void)
  {
          static GHashTable *type_table;
          if (type_table == NULL) {
                  type_table = g_hash_table_new(g_str_hash, g_str_equal);
          }
          return type_table;
  }
#+end_src
这里可以看到这个类型表的落脚点，名字就叫：type_table，是个static类型的，初始化一次为NULL，第一次运行创建这样一个表，在函数退出时依旧有效。并且可以将各种类型加入到这个表中。这样注册完成后，就可以在初始化阶段时，调用各个TypeInfo的class_init函数了，就是在type_initialize（可能递归多次）里调用class_init函数。

以上其实又涉及到了QOM的概念， 在上节已经首次提出了QOM的一方面。这里针对本节的角度再次总结QOM的另一方面。 *QOM：Qemu Object Model。所谓model，就是有一定的套路或范式，不论来多少类型、设备，都按这个注册、初始化最后具现的流程来编码，具体点就是到处设置回调函数。*

后面还会遇到QOM的体现，本文会更多的以具体代码里去阐述QOM的概念，不然只是上节的QOM概念会空洞。

现在开始分析object_class_property_add_bool，以解答前面的问题“object_property_set里怎么就能按如下方式调用set函数呢”。在这个函数中有：
#+begin_src c
  ObjectProperty *
  object_class_property_add_bool(ObjectClass *klass, const char *name,
                                 bool (*get)(Object *, Error **),
                                 void (*set)(Object *, bool, Error **))
  {
          BoolProperty *prop = g_malloc0(sizeof(*prop));
          prop->get = get;
          prop->set = set;
          return object_class_property_add(klass, name, "bool",
                                           get ? property_get_bool : NULL,
                                           set ? property_set_bool : NULL,
                                           NULL,
                                           prop);
  }
#+end_src
上面的set函数其实就是device_set_realized，这在object_class_property_add_bool的父函数device_class_init里可以看到，这里要注意，object_class_property_add_bool自己也构造了一个BoolProperty类型的prop属性，这个BoolProperty的set函数是device_set_realized，并将它作为最后一个参数传递给了object_class_property_add，这个函数在下面马上会分析，它也构造了一个prop，不过是ObjectProperty类型的。
#+begin_src c
  ObjectProperty *
  object_class_property_add(ObjectClass *klass,
                            const char *name,
                            const char *type,
                            ObjectPropertyAccessor *get,
                            ObjectPropertyAccessor *set,
                            ObjectPropertyRelease *release,
                            void *opaque)
  {
          ObjectProperty *prop;
          prop = g_malloc0(sizeof(*prop));
          prop->set = set;
          prop->opaque = opaque;
          g_hash_table_insert(klass->properties, prop->name, prop);
          return prop;
  }
#+end_src
可以看到这里调用了g_hash_table_insert函数将一个prop加入到了properties哈希表中，其key就是传进来的name字符串，为realized。这样在前面的object_property_set函数中就可以通过object_property_find_err函数，以name参数为realized找到其对应的prop，进而调用这个prop对应的set函数：property_set_bool。同时还有很重要的一点，opaque被保存在了prop->opaue成员里，这样在早先分析过的object_property_set函数里，在先找到了ObjectProperty类型的prop后，才能从这个prop里取出opaque，而opaque又作为一个BoolProperty类型的参数传递给object_property_set里调用的set函数（也就是property_set_bool），这样在这个set函数里又才调用BoolProperty的set回调为device_set_realized，进而去执行具现化的流程。

分析完device_set_realized，往下打算分析下x86_cpu_realizefn函数的调用。device_set_realized里对x86_cpu_realizefn调用是这样的：
#+begin_src c
  static void device_set_realized(Object *obj, bool value, Error **errp)
  {
          DeviceState *dev = DEVICE(obj);
          DeviceClass *dc = DEVICE_GET_CLASS(dev);
          if (dc->realize) {
                  dc->realize(dev, &local_err);
          }
  }
#+end_src

这里出现了大写的DEVICE，这点在前面提到过，也是QOM的一个方面。但是具体的DEVICE的实现，前面没有提到，这里简要分析下。

DEVICE的最开始定义在include/hw/qdev-core.h里有：
#+begin_src c
OBJECT_DECLARE_TYPE(DeviceState, DeviceClass, DEVICE)
#+end_src
#+begin_src c
  #define OBJECT_DECLARE_TYPE(InstanceType, ClassType, MODULE_OBJ_NAME) \
          typedef struct InstanceType InstanceType; \
          typedef struct ClassType ClassType; \
          \
          G_DEFINE_AUTOPTR_CLEANUP_FUNC(InstanceType, object_unref) \
          \
          DECLARE_OBJ_CHECKERS(InstanceType, ClassType, \
                               MODULE_OBJ_NAME, TYPE_##MODULE_OBJ_NAME)
#+end_src
#+begin_src c
  #define DECLARE_OBJ_CHECKERS(InstanceType, ClassType, OBJ_NAME, TYPENAME) \
          DECLARE_INSTANCE_CHECKER(InstanceType, OBJ_NAME, TYPENAME)	\
          \
          DECLARE_CLASS_CHECKERS(ClassType, OBJ_NAME, TYPENAME)
#+end_src
可以看到上节介绍过的DECLARE_INSTANCE_CHECKER，里面会有DEVICE的定义，就不再进一步贴代码了。这里主要关心下DECLARE_CLASS_CHECKERS，这个前面没有介绍过。
#+begin_src c
  define DECLARE_CLASS_CHECKERS(ClassType, OBJ_NAME, TYPENAME) \
       static inline G_GNUC_UNUSED ClassType * \
       OBJ_NAME##_GET_CLASS(const void *obj) \
  { return OBJECT_GET_CLASS(ClassType, obj, TYPENAME); } \
  \
  static inline G_GNUC_UNUSED ClassType * \
  OBJ_NAME##_CLASS(const void *klass) \
  { return OBJECT_CLASS_CHECK(ClassType, klass, TYPENAME); }
#+end_src
可以看到，OBJ_NAME被替换为OBJECT_DECLARE_TYPE宏的第三个参数为DEVICE，这样就有了device_set_realized函数里可以用DEVICE_GET_CLASS了。再往下跟OBJECT_GET_CLASS->object_get_class的话会知道，最后实际获取的就是obj->class，这里就不再贴代码了。

回到device_set_realized函数，里面最主要的就是通过dc调用了realize函数（就是x86_cpu_realizefn）了。这又是一个回调，那么这个回调在哪里设置的呢，分析这个问题，又要引出QOM的另一方面了： *object的继承* 。下面分析这个问题。

先看几个结构体：
#+begin_src c
  static const TypeInfo x86_cpu_type_info = {
          name = TYPE_X86_CPU,
          .parent = TYPE_CPU,
          .class_init = x86_cpu_common_class_init,
  };
#+end_src
#+begin_src c
  static const TypeInfo cpu_type_info = {
          .name = TYPE_CPU,
          .parent = TYPE_DEVICE,
          .class_init = cpu_class_init,
  };
#+end_src
#+begin_src c
  static const TypeInfo device_type_info = {
          .name = TYPE_DEVICE,
          .parent = TYPE_OBJECT,
          .class_init = device_class_init,
  };
#+end_src
#+begin_src c
  static const TypeInfo object_info = {
          .name = TYPE_OBJECT,
          .class_init = object_class_init,
  };
#+end_src
观察以上TypeInfo定义，可以很清楚的看到，它们构成了父子继承关系，这就是QOM对各种虚拟计算机对象的一种抽象，在类型的初始化阶段时，会递归调用type_initialize函数，就是如果一个TypeInfo如果有parent，会先对parent这个TypeImpl调用type_initialize，然后到递归的最底层时，会调用class_init（如果有的话）：
#+begin_src c
  static void type_initialize(TypeImpl *ti)
  {
          parent = type_get_parent(ti);
          if (parent) {
                  type_initialize(parent);
          }
          if (ti->class_init) {
                  ti->class_init(ti->class, ti->class_data);
          }
  }
#+end_src
而对于x86_cpu_type_info的class_init来说（x86_cpu_common_class_init），就会调用device_class_set_parent_realize来设置realize回调函数：
#+begin_src c
  static void x86_cpu_common_class_init(ObjectClass *oc, void *data)
  {
          X86CPUClass *xcc = X86_CPU_CLASS(oc);
          DeviceClass *dc = DEVICE_CLASS(oc);
          device_class_set_parent_realize(dc, x86_cpu_realizefn,
                                          &xcc->parent_realize);
  }
#+end_src
#+begin_src c
  void device_class_set_parent_realize(DeviceClass *dc,
                                       DeviceRealize dev_realize,
                                       DeviceRealize *parent_realize)
  {
          *parent_realize = dc->realize;
          dc->realize = dev_realize;
  }
#+end_src
这样设置好以后，在device_set_realized中就可以以dc->realize这样的方式调用了。至于上面具有继承关系的各个TypeInfo，它们是如何注册的，这点不再赘述，前面已经有多个例子分析到。

再往后看调用链，来到qemu_init_vcpu，里面有：
#+begin_src c
  void qemu_init_vcpu(CPUState *cpu)
  {
          cpus_accel->create_vcpu_thread(cpu);
  }
#+end_src
create_vcpu_thread（就是kvm_start_vcpu_thread）是一开始kvm_start_vcpu_thread调用链里的最后一个回调了，对于它笔者不打算详细分析了，因为到这里对于QOM的套路已经驾轻就熟了，这里只是简单列下代码并简单解释下，以验证或加深理解。以后的QEMU+KVM关于QOM的主题分析也不会这么详细了，在那些文档里，涉及到QOM的分析，都会请移步至此。

关于如何调用起create_vcpu_thread，如下一些代码所示：
#+begin_src c
main->qemu_init->qmp_x_exit_preconfig->qemu_init_board->machine_run_board_init->accel_init_interfaces->accel_init_ops_interfaces->cpus_register_accel

static const AccelOpsClass *cpus_accel;

void cpus_register_accel(const AccelOpsClass *ops)
{
        cpus_accel = ops;
}
#+end_src
#+begin_src c
  void accel_init_ops_interfaces(AccelClass *ac)
  {
          const char *ac_name;
          char *ops_name;
          ObjectClass *oc;
          AccelOpsClass *ops;
          ac_name = object_class_get_name(OBJECT_CLASS(ac));
          ops_name = g_strdup_printf("%s" ACCEL_OPS_SUFFIX, ac_name);
          oc = module_object_class_by_name(ops_name);
          ops = ACCEL_OPS_CLASS(oc);
          cpus_register_accel(ops);
  }
#+end_src
#+begin_src c
  void machine_run_board_init(MachineState *machine, const char *mem_path, Error **errp)
  {
          accel_init_interfaces(ACCEL_GET_CLASS(machine->accelerator));
  }
#+end_src
可以看到，在machine有了accelerator之后，就可以从里面取出ops交给全局静态变量cpus_accel了。

但这些还不涉及到把create_vcpu_thread设置为kvm_start_vcpu_thread。可以想见这是在object_class_foreach_tramp->type_initialize里调用class_init完成的：
#+begin_src c
  DECLARE_CLASS_CHECKERS(AccelOpsClass, ACCEL_OPS, TYPE_ACCEL_OPS)
  static void kvm_accel_ops_class_init(ObjectClass *oc, void *data)
  {
          AccelOpsClass *ops = ACCEL_OPS_CLASS(oc);
          ops->create_vcpu_thread = kvm_start_vcpu_thread;
  }
  static const TypeInfo kvm_accel_ops_type = {
          .name = ACCEL_OPS_NAME("kvm"),
          .parent = TYPE_ACCEL_OPS,
          .class_init = kvm_accel_ops_class_init,
          .abstract = true,
  };
  static void kvm_accel_ops_register_types(void)
  {
          type_register_static(&kvm_accel_ops_type);
  }
  type_init(kvm_accel_ops_register_types);
#+end_src
看到type_init一切就明晰了。

到目前为止，本文花了很多篇幅去阐述QOM，还较少涉及虚拟化本身，笔者认为这是必要的，从某种角度来说，QOM就代表了QEMU代码的组织，并且弄清楚这种组织或调用关系有可能比虚拟化本身更费劲。但对QOM的清晰，会有助于对QEMU代码的全局把握。当然虚拟化本身后面肯定还会深入的。

以上着重对QOM进行了介绍， *从现在开始，会更多的偏向于虚拟化本身了* ，在本节也就是QEMU里对于CPU的创建到底做了哪些事情。

在本节一开始，列出了创建VCPU的函数流程，只是到目前都只关注了这个调用流程是怎么调用起来的（在哪里设置回调，在哪里调用回调）．下面再贴下这个函数流程，后续就是主要关注这些流程上函数的逻辑了：
: main->qemu_init->qmp_x_exit_preconfig->qemu_init_board->machine_run_board_init->pc_init1->x86_cpus_init->x86_cpu_new->qdev_realize->object_property_set_bool->object_property_set_qobject->object_property_set->
: property_set_bool->device_set_realized->x86_cpu_realizefn->qemu_init_vcpu->kvm_start_vcpu_thread

从machine_run_board_init调用pc_init1说起。QEMU中，主板模型有两种，一是i440fx，另外一个是q35，本文主要基于前者研究。不论是i440fx还是q35，都要通过DEFINE_PC_MACHINE去定义MachineClass的init函数：
#+begin_src c
#define DEFINE_I440FX_MACHINE(suffix, name, compatfn, optionfn) \
    static void pc_init_##suffix(MachineState *machine) \
    { \
        void (*compat)(MachineState *m) = (compatfn); \
        if (compat) { \
            compat(machine); \
        } \
        pc_init1(machine, TYPE_I440FX_PCI_HOST_BRIDGE, \
                 TYPE_I440FX_PCI_DEVICE); \
    } \
    DEFINE_PC_MACHINE(suffix, name, pc_init_##suffix, optionfn)
#+end_src
#+begin_src c
#define DEFINE_PC_MACHINE(suffix, namestr, initfn, optsfn) \
    static void pc_machine_##suffix##_class_init(ObjectClass *oc, void *data) \
    { \
        MachineClass *mc = MACHINE_CLASS(oc); \
        optsfn(mc); \
        mc->init = initfn; \
        mc->kvm_type = pc_machine_kvm_type; \
    } \
    static const TypeInfo pc_machine_type_##suffix = { \
        .name       = namestr TYPE_MACHINE_SUFFIX, \
        .parent     = TYPE_PC_MACHINE, \
        .class_init = pc_machine_##suffix##_class_init, \
    }; \
    static void pc_machine_init_##suffix(void) \
    { \
        type_register(&pc_machine_type_##suffix); \
    } \
    type_init(pc_machine_init_##suffix)
#+end_src
对于q35的话就是宏DEFINE_Q35_MACHINE。这里suffix其实就是i440fx主板的版本，比如v8_2、v8_1等。可以看到initfn其实主要就是调用了pc_init1，而initfn又是作为MachineClass的init回调，所以在machine_run_board_init的最后可以这样调用：
#+begin_src c
machine_class->init(machine);
#+end_src

下面继续分析pc_init1调用的x86_cpus_init函数，该函数的最后会调用x86_cpu_new：
#+begin_src c
    possible_cpus = mc->possible_cpu_arch_ids(ms);
    for (i = 0; i < ms->smp.cpus; i++) {
        x86_cpu_new(x86ms, possible_cpus->cpus[i].arch_id, &error_fatal);
    }
#+end_src
可以看到这里先调用了possible_cpu_arch_ids函数去返回一个CPUArchIDList类型的指针，该类型里主要是存放了当前所有可用CPU的arch_id以及其它一些信息。possible_cpu_arch_ids函数对于x86架构就是x86_possible_cpu_arch_ids函数，这是在x86_machine_class_init函数中设置的回调。在x86_possible_cpu_arch_ids中可以看到，返回的CPUArchIdList实际就来自ms->possible_cpus成员，第一次调用x86_possible_cpu_arch_ids时，possible_cpus指针是没有空间的，x86_possible_cpu_arch_ids第一次被调用时，会使用g_malloc0给ms->possible_cpus分配空间。同时在这个函数里还会使用ms->smp.max_cpus获取当前的最大可用cpu数目，这个smp.max_cpus在machine_parse_smp_config函数中通过用户配置获得，比如-smp参数设置为16，那么
x86_possible_cpu_arch_ids中看到的smp.max_cpus可能就是这个16。x86_possible_cpu_arch_ids中最重要的逻辑就是为每个cpu构建arch_id，这个arch_id其实就是个uint32_t类型，32位的数值，里面按比特位存放了pkg_id、die_id、core_id以及smt_id。这里有必要简单的介绍下cpu的拓扑结构，看两个结构体就明白了：
#+begin_src c
typedef uint32_t apic_id_t;

typedef struct X86CPUTopoIDs {
    unsigned pkg_id;
    unsigned die_id;
    unsigned core_id;
    unsigned smt_id;
} X86CPUTopoIDs;

typedef struct X86CPUTopoInfo {
    unsigned dies_per_pkg;
    unsigned cores_per_die;
    unsigned threads_per_core;
} X86CPUTopoInfo;
#+end_src
可以很清楚的看到处理器的拓扑结构，首先处理器有若干个package，一个package下有若干个dies，而每个die下又有若干个cores，core下又有若干个threads，最下层又分是否是否支持超线程（SMT，Simultaneous Multi-threading）从而有smt_id，每个id成员其实就是指明自己在哪个层级的id号。

现在回到x86_possible_cpu_arch_ids函数，继续研究arch_id是如何构建的：
#+begin_src c
  for (i = 0; i < ms->possible_cpus->len; i++) {
          ms->possible_cpus->cpus[i].arch_id =
                  x86_cpu_apic_id_from_index(x86ms, i);
   }

  uint32_t x86_cpu_apic_id_from_index(X86MachineState *x86ms,
                                      unsigned int cpu_index)
  {
          X86CPUTopoInfo topo_info;

          init_topo_info(&topo_info, x86ms);

          return x86_apicid_from_cpu_idx(&topo_info, cpu_index);
  }

  static void init_topo_info(X86CPUTopoInfo *topo_info,
                             const X86MachineState *x86ms)
  {
          MachineState *ms = MACHINE(x86ms);

          topo_info->dies_per_pkg = ms->smp.dies;
          topo_info->cores_per_die = ms->smp.cores;
          topo_info->threads_per_core = ms->smp.threads;
  }
#+end_src
可以看到，函数x86_cpu_apic_id_from_index用于构造arch_id，该函数首先使用init_topo_info初始化一个topo_info结构体，这个结构体里有一些成员，会记录每个pkg有多少die，每个die又有多少个core，每个core又有多少个thread，这些成员的值都来自machine_parse_smp_config函数，这个函数前面提到过，都是依据用户的配置填充便是。

继续看x86_apicid_from_cpu_idx函数：
#+begin_src c
static inline apic_id_t x86_apicid_from_cpu_idx(X86CPUTopoInfo *topo_info,
                                                unsigned cpu_index)
{
    X86CPUTopoIDs topo_ids;
    x86_topo_ids_from_idx(topo_info, cpu_index, &topo_ids);
    return x86_apicid_from_topo_ids(topo_info, &topo_ids);
}

static inline void x86_topo_ids_from_idx(X86CPUTopoInfo *topo_info,
                                         unsigned cpu_index,
                                         X86CPUTopoIDs *topo_ids)
{
    unsigned nr_dies = topo_info->dies_per_pkg;
    unsigned nr_cores = topo_info->cores_per_die;
    unsigned nr_threads = topo_info->threads_per_core;

    topo_ids->pkg_id = cpu_index / (nr_dies * nr_cores * nr_threads);
    topo_ids->die_id = cpu_index / (nr_cores * nr_threads) % nr_dies;
    topo_ids->core_id = cpu_index / nr_threads % nr_cores;
    topo_ids->smt_id = cpu_index % nr_threads;
}
#+end_src

x86_apicid_from_cpu_idx调用了x86_topo_ids_from_idx初始化一个topo_ids结构体，该结构体记录了针对现在传进来的cpu_index（实际就是for循环里的循环变量），其对应的pkg_id、die_id、core_id以及smt_id各是多少，注意这些id在x86_topo_ids_from_idx函数里是如何计算的，针对每级id，要计算其下的级别共能容纳多少，然后除以这个数字得到的值再对本级能容纳多少取余得到了本级的id。

有了各级别的id存于topo_ids里就可以调用x86_apicid_from_topo_ids了：
#+begin_src c
static inline apic_id_t x86_apicid_from_topo_ids(X86CPUTopoInfo *topo_info,
                                                 const X86CPUTopoIDs *topo_ids)
{
    return (topo_ids->pkg_id  << apicid_pkg_offset(topo_info)) |
           (topo_ids->die_id  << apicid_die_offset(topo_info)) |
           (topo_ids->core_id << apicid_core_offset(topo_info)) |
           topo_ids->smt_id;
}


#+end_src
在这里就可以看到各级id左移拼接在一起形成一个apic_id，至于apicid_xxx_offset的代码细节就不分析了，原理就是左移出来下级所有id需要的bit位就可以了。

x86_possible_cpu_arch_ids再后面的逻辑就是填充possible_cpus里各个cpu属性（props）的各级id了，就不详细分析了。

介绍完了x86_possible_cpu_arch_ids，得到了possible_cpus，现在回到x86_cpus_init，这时就可以使用possible_cpus->cpus[i].arch_id去调用x86_cpu_new函数了，x86_cpu_new函数定义如下：
#+begin_src c
void x86_cpu_new(X86MachineState *x86ms, int64_t apic_id, Error **errp)
{
    Object *cpu = object_new(MACHINE(x86ms)->cpu_type);

    if (!object_property_set_uint(cpu, "apic-id", apic_id, errp)) {
        goto out;
    }
    qdev_realize(DEVICE(cpu), NULL, errp);

out:
    object_unref(cpu);
}
#+end_src
object_new函数定义如下：
#+begin_src c
Object *object_new(const char *typename)
{
    TypeImpl *ti = type_get_by_name(typename);

    return object_new_with_type(ti);
}
#+end_src
从object_new函数定义可以看到，type_get_by_name里根据传入的字符串名（cpu_type或typename，在x86-64平台这个串就是qemu64-x86_64-cpu）找到TypeImpl，那么可以想见对应typename的TypeImpl先前就已经放入到type_table哈希表里了，这个流程如下：
#+begin_src
main->qemu_init->qemu_init_subsystems->module_call_init->x86_cpu_register_types->x86_register_cpudef_types->x86_register_cpu_model_type->type_register->type_register_internal->type_table_add
#+end_src
注意用于注册到type_table哈希表的TypeInfo在x86_register_cpu_model_type里初始化：
#+begin_src c
static void x86_register_cpu_model_type(const char *name, X86CPUModel *model)
{
    g_autofree char *typename = x86_cpu_type_name(name);
    TypeInfo ti = {
        .name = typename,
        .parent = TYPE_X86_CPU,
        .class_init = x86_cpu_cpudef_class_init,
        .class_data = model,
    };

    type_register(&ti);
}
#+end_src
注意这里ti是个局部变量，插入到type_table的当然不可能就是这个ti，因为局部变量在x86_register_cpu_model_type函数返回时就会销毁，真正插入到type_table的TypeInfo在type_register_internal中调用type_new里会使用malloc函数申请内存，而这里定义的局部ti会在type_new里作为一个构建插入到type_table中ti的信息来源。

查看这个局部ti的定义，可以看到被初始化的成员很有限，尤其是在object_new函数里调用object_new_with_type时，要使用instance_size成员作为参数调用g_malloc申请内存，该成员在定义TypeInfo类型的ti变量时没有被初始化，这个成员的初始化流程如下：
#+begin_src
main->qemu_init->qemu_create_machine->select_machine->object_class_get_list->object_class_foreach->g_hash_table_foreach->object_class_foreach_tramp->type_initialize
#+end_src
type_initialize里有如下代码：
#+begin_src c
ti->instance_size = type_object_get_size(ti);
#+end_src
#+begin_src c
static size_t type_object_get_size(TypeImpl *ti)
{
    if (ti->instance_size) {
        return ti->instance_size;
    }

    if (type_has_parent(ti)) {
        return type_object_get_size(type_get_parent(ti));
    }

    return 0;
}
#+end_src
可以看到是在类型初始化时设置instance_size成员，其实际是父TypeImpl的大小，还有很多成员都来自父TypeImpl。在前面x86_register_cpu_model_type函数中定义局部变量ti时，就有parent了，其为TYPE_X86_CPU，就是串x86_64-cpu，这对应的TypeImpl其实就是x86_cpu_type_info，具体定义这里就不展开了。

回到object_new函数，现在终于搞清楚了这个函数的第一行变量ti的来龙去脉，在object_new就可以继续调用object_new_with_type，在这里面可以使用得到的ti里的信息去malloc出cpu Object，并返回到x86_cpu_new函数里。

回到x86_cpu_new继续分析，随后调用了object_property_set_uint来设置cpu的apic-id属性为刚生成的apic_id。

现在详细分析object_property_set_uint函数，按照上面说的它的作用，对它的分析从两个方面来进行，一是apic-id这个属性的设置流程，二是对这个设的值本身进行的操作。

先看第一个方面，有一个下面调用的流程：
: x86_cpu_new->object_property_set_uint->object_property_set_qobject->object_property_set

最后的object_property_set里面有下面的代码：
#+begin_src c
prop->set(obj, v, name, prop->opaque, errp);
#+end_src

所谓第一个方面，就是研究上面的set函数是怎么被设置，进而在这里可以调用起来。x86架构有个Property数组叫x86_cpu_properties，里面定义了所有X86 cpu可能有的Property，针对apic-id属性使用DEFINE_PROP_UINT32宏定义一个类型为Property数组元素：

* KVM CPU创建

* VCPU的运行

* VCPU的调度
